{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d9e384",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89f8997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qwen2.5-VL 3B regression fine-tuning (image + caption → alignment_score)\n",
    "# ✅ LoRA (FEATURE_EXTRACTION)\n",
    "# ✅ Single-process / single-GPU\n",
    "# ✅ Lots of progress/debug printing\n",
    "# ✅ Fixes common CUDA device-side assert causes:\n",
    "#    - GPU poisoned -> restart kernel\n",
    "#    - unexpected keys passed to model\n",
    "#    - overly long sequences (truncation)\n",
    "#    - bf16 instability -> fp16\n",
    "#    - fp16 loss instability -> compute MSE in fp32\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 0) (Optional) install deps\n",
    "# %%\n",
    "# !pip -q install -U \"transformers>=4.45\" datasets accelerate peft pillow\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 1) IMPORTANT: env + cache (RUN FIRST; restart kernel if you previously imported torch/transformers)\n",
    "# %%\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Make CUDA errors synchronous (must be set before CUDA init / torch import for best results)\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "HF_CACHE_ROOT = \"/work/hdd/bfrc\"\n",
    "os.environ[\"HF_HOME\"] = str(Path(HF_CACHE_ROOT) / \"hf_home\")\n",
    "os.environ[\"HUGGINGFACE_HUB_CACHE\"] = str(Path(HF_CACHE_ROOT) / \"hub\")\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = str(Path(HF_CACHE_ROOT) / \"transformers\")\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = str(Path(HF_CACHE_ROOT) / \"datasets\")\n",
    "CACHE_DIR = str(Path(HF_CACHE_ROOT) / \"cache_dir\")\n",
    "\n",
    "for p in [\n",
    "    os.environ[\"HF_HOME\"],\n",
    "    os.environ[\"HUGGINGFACE_HUB_CACHE\"],\n",
    "    os.environ[\"TRANSFORMERS_CACHE\"],\n",
    "    os.environ[\"HF_DATASETS_CACHE\"],\n",
    "    CACHE_DIR,\n",
    "]:\n",
    "    Path(p).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"[env] CUDA_LAUNCH_BLOCKING:\", os.environ[\"CUDA_LAUNCH_BLOCKING\"])\n",
    "print(\"[cache] HF_HOME:\", os.environ[\"HF_HOME\"])\n",
    "print(\"[cache] HUGGINGFACE_HUB_CACHE:\", os.environ[\"HUGGINGFACE_HUB_CACHE\"])\n",
    "print(\"[cache] TRANSFORMERS_CACHE:\", os.environ[\"TRANSFORMERS_CACHE\"])\n",
    "print(\"[cache] HF_DATASETS_CACHE:\", os.environ[\"HF_DATASETS_CACHE\"])\n",
    "print(\"[cache] CACHE_DIR:\", CACHE_DIR)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 2) Config\n",
    "# %%\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "TRAIN_CSV = \"\"\n",
    "EVAL_CSV  = \"\"\n",
    "IMG_ROOT  = \"\"\n",
    "\n",
    "MODEL_ID = \"Qwen/Qwen2.5-VL-3B-Instruct\"\n",
    "\n",
    "OUTPUT_DIR = str(Path(HF_CACHE_ROOT) / \"finetunes\" / \"qwen2vl-regression-out\")\n",
    "Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# training\n",
    "EPOCHS = 2\n",
    "PER_DEVICE_BATCH_SIZE = 4\n",
    "GRAD_ACCUM_STEPS = 8\n",
    "LR = 2e-4\n",
    "WARMUP_RATIO = 0.03\n",
    "WEIGHT_DECAY = 0.0\n",
    "MAX_STEPS = -1\n",
    "\n",
    "# prompt + truncation\n",
    "PROMPT_PREFIX = \"Predict the alignment score (a real number). Caption:\"\n",
    "MAX_TEXT_LEN = 2048  # try 1024 if still unstable/oom\n",
    "\n",
    "# LoRA\n",
    "LORA_R = 16\n",
    "LORA_ALPHA = 32\n",
    "LORA_DROPOUT = 0.05\n",
    "\n",
    "SEED = 42\n",
    "NUM_WORKERS = 2\n",
    "\n",
    "print(\"[cfg] OUTPUT_DIR:\", OUTPUT_DIR)\n",
    "print(\"[cfg] CUDA:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"[cfg] GPU:\", torch.cuda.get_device_name(0))\n",
    "    print(\"[cfg] device_count:\", torch.cuda.device_count())\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 3) Load dataset\n",
    "# %%\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"[data] Loading CSV(s)...\")\n",
    "data_files = {\"train\": TRAIN_CSV}\n",
    "if EVAL_CSV:\n",
    "    data_files[\"eval\"] = EVAL_CSV\n",
    "\n",
    "raw = load_dataset(\"csv\", data_files=data_files, cache_dir=CACHE_DIR)\n",
    "\n",
    "print(\"[data] splits:\", list(raw.keys()))\n",
    "print(\"[data] columns:\", raw[\"train\"].column_names)\n",
    "print(\"[data] train rows:\", len(raw[\"train\"]))\n",
    "if EVAL_CSV:\n",
    "    print(\"[data] eval rows:\", len(raw[\"eval\"]))\n",
    "\n",
    "required_cols = {\"img_name\", \"caption\", \"alignment_score\"}\n",
    "for split in raw.keys():\n",
    "    missing = required_cols - set(raw[split].column_names)\n",
    "    if missing:\n",
    "        raise ValueError(f\"{split} is missing columns: {missing}\")\n",
    "\n",
    "print(\"[data] example row:\", raw[\"train\"][0])\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 4) Torch dataset (image load on the fly)\n",
    "# %%\n",
    "import os\n",
    "import math\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "def load_image(path: str) -> Image.Image:\n",
    "    with Image.open(path) as im:\n",
    "        return im.convert(\"RGB\")\n",
    "\n",
    "class CSVImageCaptionRegressionDataset(Dataset):\n",
    "    def __init__(self, hf_dataset, img_root: str):\n",
    "        self.ds = hf_dataset\n",
    "        self.img_root = img_root\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.ds[idx]\n",
    "        img_path = os.path.join(self.img_root, str(row[\"img_name\"]))\n",
    "        caption = str(row[\"caption\"])\n",
    "        label = float(row[\"alignment_score\"])\n",
    "\n",
    "        if not math.isfinite(label):\n",
    "            raise ValueError(f\"Non-finite label at idx={idx}: {label}\")\n",
    "\n",
    "        image = load_image(img_path)\n",
    "        return {\"image\": image, \"caption\": caption, \"labels\": label, \"img_path\": img_path}\n",
    "\n",
    "train_ds = CSVImageCaptionRegressionDataset(raw[\"train\"], IMG_ROOT)\n",
    "eval_ds  = CSVImageCaptionRegressionDataset(raw[\"eval\"], IMG_ROOT) if EVAL_CSV else None\n",
    "print(\"[ds] train:\", len(train_ds), \"| eval:\", (len(eval_ds) if eval_ds else 0))\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 5) Load processor + base model (fp16) + regression head + LoRA (FEATURE_EXTRACTION)\n",
    "# %%\n",
    "from transformers import AutoProcessor, AutoModel\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "print(\"[model] Loading processor...\")\n",
    "processor = AutoProcessor.from_pretrained(MODEL_ID, trust_remote_code=True, cache_dir=CACHE_DIR)\n",
    "\n",
    "print(\"[model] Loading base model (fp16)...\")\n",
    "base = AutoModel.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    trust_remote_code=True,\n",
    "    cache_dir=CACHE_DIR,\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "print(\"[model] Base loaded.\")\n",
    "\n",
    "class Qwen2VLForRegression(nn.Module):\n",
    "    def __init__(self, base_model: nn.Module):\n",
    "        super().__init__()\n",
    "        self.base = base_model\n",
    "        hidden_size = getattr(base_model.config, \"hidden_size\", None)\n",
    "        if hidden_size is None:\n",
    "            hidden_size = getattr(getattr(base_model.config, \"text_config\", None), \"hidden_size\", None)\n",
    "        if hidden_size is None:\n",
    "            raise ValueError(\"Could not infer hidden_size from model config.\")\n",
    "        self.reg_head = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, labels=None, **inputs):\n",
    "        # Only pass tensor inputs relevant to the base model (safety).\n",
    "        base_inputs = {k: v for k, v in inputs.items() if torch.is_tensor(v) and k != \"labels\"}\n",
    "\n",
    "        out = self.base(**base_inputs, output_hidden_states=False, return_dict=True)\n",
    "        last_hidden = out.last_hidden_state  # [B, T, H]\n",
    "\n",
    "        attn = base_inputs.get(\"attention_mask\", None)\n",
    "        if attn is None:\n",
    "            pooled = last_hidden.mean(dim=1)\n",
    "        else:\n",
    "            attn = attn.to(last_hidden.dtype)\n",
    "            denom = attn.sum(dim=1).clamp_min(1.0).unsqueeze(-1)\n",
    "            pooled = (last_hidden * attn.unsqueeze(-1)).sum(dim=1) / denom\n",
    "\n",
    "        pred = self.reg_head(pooled).squeeze(-1)  # [B]\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # fp32 loss for stability\n",
    "            loss = F.mse_loss(pred.float(), labels.float())\n",
    "\n",
    "        return {\"loss\": loss, \"predictions\": pred}\n",
    "\n",
    "model = Qwen2VLForRegression(base)\n",
    "\n",
    "print(\"[lora] Applying LoRA (FEATURE_EXTRACTION)...\")\n",
    "target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    "lora_cfg = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    target_modules=target_modules,\n",
    "    task_type=TaskType.FEATURE_EXTRACTION,\n",
    ")\n",
    "model.base = get_peft_model(model.base, lora_cfg)\n",
    "model.base.print_trainable_parameters()\n",
    "\n",
    "for p in model.reg_head.parameters():\n",
    "    p.requires_grad = True\n",
    "print(\"[model] reg_head params:\", sum(p.numel() for p in model.reg_head.parameters()))\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 6) Collator (NO extra keys passed into Trainer), with truncation + token-range checks\n",
    "# %%\n",
    "from typing import Any, Dict, List\n",
    "import torch\n",
    "\n",
    "class Qwen2VLRegressionCollator:\n",
    "    def __init__(self, processor, prompt_prefix: str, max_text_len: int, debug: bool = False):\n",
    "        self.processor = processor\n",
    "        self.prompt_prefix = prompt_prefix\n",
    "        self.max_text_len = max_text_len\n",
    "        self.debug = debug\n",
    "        self._seen = 0\n",
    "\n",
    "        # best-effort vocab size for safety checks\n",
    "        self.vocab_size = None\n",
    "        tok = getattr(processor, \"tokenizer\", None)\n",
    "        if tok is not None and hasattr(tok, \"vocab_size\"):\n",
    "            self.vocab_size = int(len(tok))  # includes added/special tokens\n",
    "\n",
    "\n",
    "    def __call__(self, batch: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        images = [ex[\"image\"] for ex in batch]\n",
    "        captions = [ex[\"caption\"] for ex in batch]\n",
    "        labels = torch.tensor([float(ex[\"labels\"]) for ex in batch], dtype=torch.float32)\n",
    "\n",
    "        texts = []\n",
    "        for cap in captions:\n",
    "            prompt = f\"{self.prompt_prefix}\\n{cap}\\nScore:\"\n",
    "            messages = [{\"role\": \"user\", \"content\": [{\"type\": \"image\"}, {\"type\": \"text\", \"text\": prompt}]}]\n",
    "            text = self.processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "            texts.append(text)\n",
    "\n",
    "        model_inputs = self.processor(\n",
    "            text=texts,\n",
    "            images=images,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=self.max_text_len,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        model_inputs[\"labels\"] = labels\n",
    "\n",
    "        # Safety checks that often catch device-side asserts early\n",
    "        if \"input_ids\" in model_inputs:\n",
    "            max_id = int(model_inputs[\"input_ids\"].max())\n",
    "            if self.vocab_size is not None and max_id >= self.vocab_size:\n",
    "                # This would cause embedding index OOB -> device-side assert\n",
    "                raise ValueError(f\"input_ids has id {max_id} >= vocab_size {self.vocab_size}\")\n",
    "\n",
    "        if self.debug and self._seen < 3:\n",
    "            print(\"[collator] prompt preview:\\n\", texts[0][:400], \"...\")\n",
    "            print(\"[collator] input_ids:\", tuple(model_inputs[\"input_ids\"].shape))\n",
    "            print(\"[collator] labels:\", labels[: min(4, len(labels))].tolist())\n",
    "            self._seen += 1\n",
    "\n",
    "        return model_inputs\n",
    "\n",
    "collator = Qwen2VLRegressionCollator(processor, PROMPT_PREFIX, MAX_TEXT_LEN, debug=False)\n",
    "\n",
    "# sanity check\n",
    "b = collator([train_ds[0]])\n",
    "print(\"[collator] keys:\", list(b.keys()))\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 7) Trainer + progress prints\n",
    "# %%\n",
    "from transformers import TrainingArguments, Trainer, set_seed, TrainerCallback\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    preds, labels = eval_pred\n",
    "    preds = np.asarray(preds, dtype=np.float32)\n",
    "    labels = np.asarray(labels, dtype=np.float32)\n",
    "    mse = float(np.mean((preds - labels) ** 2))\n",
    "    mae = float(np.mean(np.abs(preds - labels)))\n",
    "    var = float(np.var(labels))\n",
    "    r2 = float(\"nan\") if var == 0.0 else float(1.0 - mse / var)\n",
    "    return {\"mse\": mse, \"mae\": mae, \"r2\": r2}\n",
    "\n",
    "class PrintProgressCallback(TrainerCallback):\n",
    "    def __init__(self, trainer_ref=None, sample_every_steps: int = 100):\n",
    "        self.trainer_ref = trainer_ref\n",
    "        self.sample_every_steps = sample_every_steps\n",
    "        self.t0 = None\n",
    "\n",
    "    def on_train_begin(self, args, state, control, **kwargs):\n",
    "        self.t0 = time.time()\n",
    "        print(f\"[train] begin | max_steps={state.max_steps} | epochs={args.num_train_epochs}\")\n",
    "\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if not logs:\n",
    "            return\n",
    "        elapsed = (time.time() - self.t0) if self.t0 else 0.0\n",
    "        msg = f\"[log] step={state.global_step} epoch={state.epoch:.3f} elapsed={elapsed/60:.1f}m\"\n",
    "        if \"loss\" in logs:\n",
    "            msg += f\" loss={logs['loss']:.4f}\"\n",
    "        if \"learning_rate\" in logs:\n",
    "            msg += f\" lr={logs['learning_rate']:.2e}\"\n",
    "        if \"grad_norm\" in logs:\n",
    "            msg += f\" grad_norm={logs['grad_norm']:.3f}\"\n",
    "        print(msg)\n",
    "\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        if self.trainer_ref is None:\n",
    "            return\n",
    "        if state.global_step > 0 and (state.global_step % self.sample_every_steps == 0):\n",
    "            try:\n",
    "                m = self.trainer_ref.model\n",
    "                m.eval()\n",
    "                ex = train_ds[0]\n",
    "                batch = collator([ex])\n",
    "                device = next(m.parameters()).device\n",
    "                batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n",
    "                with torch.no_grad():\n",
    "                    out = m(**batch)\n",
    "                pred = float(out[\"predictions\"].detach().float().cpu().item())\n",
    "                print(f\"[sample] step={state.global_step} pred={pred:.4f} label={float(ex['labels']):.4f}\")\n",
    "            except Exception as e:\n",
    "                print(\"[sample] failed:\", repr(e))\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=PER_DEVICE_BATCH_SIZE,\n",
    "    per_device_eval_batch_size=PER_DEVICE_BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRAD_ACCUM_STEPS,\n",
    "    learning_rate=LR,\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    logging_steps=10,\n",
    "    save_steps=200,\n",
    "    eval_steps=200 if eval_ds is not None else None,\n",
    "    eval_strategy=\"steps\" if eval_ds is not None else \"no\",\n",
    "    save_total_limit=2,\n",
    "    bf16=False,\n",
    "    fp16=True,\n",
    "    dataloader_num_workers=NUM_WORKERS,\n",
    "    remove_unused_columns=False,\n",
    "    max_steps=MAX_STEPS,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    data_collator=collator,\n",
    "    compute_metrics=compute_metrics if eval_ds is not None else None,\n",
    ")\n",
    "\n",
    "trainer.add_callback(PrintProgressCallback(trainer_ref=trainer, sample_every_steps=100))\n",
    "\n",
    "print(\"[train] Starting training...\")\n",
    "trainer.train()\n",
    "print(\"[train] Training finished.\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 8) Save\n",
    "# %%\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"[save] Saving full model + processor to:\", OUTPUT_DIR)\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "processor.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "lora_dir = str(Path(OUTPUT_DIR) / \"lora_adapter\")\n",
    "print(\"[save] Saving LoRA adapter to:\", lora_dir)\n",
    "model.base.save_pretrained(lora_dir)\n",
    "\n",
    "print(\"[save] Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13b37a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qwen2.5-VL 3B regression fine-tuning (image + caption -> alignment_score)\n",
    "# ✅ LoRA (FEATURE_EXTRACTION)\n",
    "# ✅ Single-process / single-GPU\n",
    "# ✅ Resume from latest checkpoint automatically\n",
    "# ✅ Validation during training (every eval_steps)\n",
    "# ✅ Stability fixes: fp32 pooling + fp32 head + fp32 MSE loss\n",
    "# ✅ Trainer-compatible outputs: returns \"logits\" (not \"predictions\")\n",
    "\n",
    "# (Optional) install deps:\n",
    "# !pip -q install -U \"transformers>=4.45\" datasets accelerate peft pillow numpy\n",
    "\n",
    "# ----------------------------\n",
    "# 1) IMPORTANT: env + cache\n",
    "# ----------------------------\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "HF_CACHE_ROOT = \"/work/hdd/bfrc\"\n",
    "os.environ[\"HF_HOME\"] = str(Path(HF_CACHE_ROOT) / \"hf_home\")\n",
    "os.environ[\"HUGGINGFACE_HUB_CACHE\"] = str(Path(HF_CACHE_ROOT) / \"hub\")\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = str(Path(HF_CACHE_ROOT) / \"transformers\")\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = str(Path(HF_CACHE_ROOT) / \"datasets\")\n",
    "CACHE_DIR = str(Path(HF_CACHE_ROOT) / \"cache_dir\")\n",
    "\n",
    "for p in [\n",
    "    os.environ[\"HF_HOME\"],\n",
    "    os.environ[\"HUGGINGFACE_HUB_CACHE\"],\n",
    "    os.environ[\"TRANSFORMERS_CACHE\"],\n",
    "    os.environ[\"HF_DATASETS_CACHE\"],\n",
    "    CACHE_DIR,\n",
    "]:\n",
    "    Path(p).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"[env] CUDA_LAUNCH_BLOCKING:\", os.environ[\"CUDA_LAUNCH_BLOCKING\"])\n",
    "print(\"[cache] HF_HOME:\", os.environ[\"HF_HOME\"])\n",
    "print(\"[cache] HUGGINGFACE_HUB_CACHE:\", os.environ[\"HUGGINGFACE_HUB_CACHE\"])\n",
    "print(\"[cache] TRANSFORMERS_CACHE:\", os.environ[\"TRANSFORMERS_CACHE\"])\n",
    "print(\"[cache] HF_DATASETS_CACHE:\", os.environ[\"HF_DATASETS_CACHE\"])\n",
    "print(\"[cache] CACHE_DIR:\", CACHE_DIR)\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Config\n",
    "# ----------------------------\n",
    "import torch\n",
    "\n",
    "TRAIN_CSV = \n",
    "EVAL_CSV  = \n",
    "IMG_ROOT  = \"\"\n",
    "\n",
    "MODEL_ID = \"Qwen/Qwen2.5-VL-3B-Instruct\"\n",
    "\n",
    "OUTPUT_DIR = str(Path(HF_CACHE_ROOT) / \"finetunes\" / \"qwen2vl-regression-out\")\n",
    "Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# training\n",
    "EPOCHS = 2\n",
    "PER_DEVICE_BATCH_SIZE = 4\n",
    "GRAD_ACCUM_STEPS = 8\n",
    "LR = 2e-4\n",
    "WARMUP_RATIO = 0.03\n",
    "WEIGHT_DECAY = 0.0\n",
    "MAX_STEPS = -1\n",
    "\n",
    "# prompt + truncation\n",
    "PROMPT_PREFIX = \"Predict the alignment score (a real number). Caption:\"\n",
    "MAX_TEXT_LEN = 2048  # try 1024 if still unstable/oom\n",
    "\n",
    "# LoRA\n",
    "LORA_R = 16\n",
    "LORA_ALPHA = 32\n",
    "LORA_DROPOUT = 0.05\n",
    "\n",
    "SEED = 42\n",
    "NUM_WORKERS = 2\n",
    "\n",
    "print(\"[cfg] OUTPUT_DIR:\", OUTPUT_DIR)\n",
    "print(\"[cfg] CUDA:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"[cfg] GPU:\", torch.cuda.get_device_name(0))\n",
    "    print(\"[cfg] device_count:\", torch.cuda.device_count())\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Load dataset\n",
    "# ----------------------------\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"[data] Loading CSV(s)...\")\n",
    "data_files = {\"train\": TRAIN_CSV}\n",
    "if EVAL_CSV:\n",
    "    data_files[\"eval\"] = EVAL_CSV\n",
    "\n",
    "raw = load_dataset(\"csv\", data_files=data_files, cache_dir=CACHE_DIR)\n",
    "\n",
    "print(\"[data] splits:\", list(raw.keys()))\n",
    "print(\"[data] columns:\", raw[\"train\"].column_names)\n",
    "print(\"[data] train rows:\", len(raw[\"train\"]))\n",
    "if EVAL_CSV:\n",
    "    print(\"[data] eval rows:\", len(raw[\"eval\"]))\n",
    "\n",
    "required_cols = {\"img_name\", \"caption\", \"alignment_score\"}\n",
    "for split in raw.keys():\n",
    "    missing = required_cols - set(raw[split].column_names)\n",
    "    if missing:\n",
    "        raise ValueError(f\"{split} is missing columns: {missing}\")\n",
    "\n",
    "print(\"[data] example row:\", raw[\"train\"][0])\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Torch dataset (image load on the fly)\n",
    "# ----------------------------\n",
    "import math\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "def load_image(path: str) -> Image.Image:\n",
    "    with Image.open(path) as im:\n",
    "        return im.convert(\"RGB\")\n",
    "\n",
    "class CSVImageCaptionRegressionDataset(Dataset):\n",
    "    def __init__(self, hf_dataset, img_root: str):\n",
    "        self.ds = hf_dataset\n",
    "        self.img_root = img_root\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.ds[idx]\n",
    "        img_path = os.path.join(self.img_root, str(row[\"img_name\"]))\n",
    "        caption = str(row[\"caption\"])\n",
    "        label = float(row[\"alignment_score\"])\n",
    "\n",
    "        if not math.isfinite(label):\n",
    "            raise ValueError(f\"Non-finite label at idx={idx}: {label}\")\n",
    "\n",
    "        image = load_image(img_path)\n",
    "        return {\"image\": image, \"caption\": caption, \"labels\": label, \"img_path\": img_path}\n",
    "\n",
    "train_ds = CSVImageCaptionRegressionDataset(raw[\"train\"], IMG_ROOT)\n",
    "eval_ds  = CSVImageCaptionRegressionDataset(raw[\"eval\"], IMG_ROOT) if EVAL_CSV else None\n",
    "print(\"[ds] train:\", len(train_ds), \"| eval:\", (len(eval_ds) if eval_ds else 0))\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Load processor + base model + regression head + LoRA\n",
    "# ----------------------------\n",
    "from transformers import AutoProcessor, AutoModel\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "print(\"[model] Loading processor...\")\n",
    "processor = AutoProcessor.from_pretrained(MODEL_ID, trust_remote_code=True, cache_dir=CACHE_DIR)\n",
    "\n",
    "print(\"[model] Loading base model (fp16)...\")\n",
    "base = AutoModel.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    trust_remote_code=True,\n",
    "    cache_dir=CACHE_DIR,\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "print(\"[model] Base loaded.\")\n",
    "\n",
    "class Qwen2VLForRegression(nn.Module):\n",
    "    \"\"\"\n",
    "    Trainer-friendly regression wrapper:\n",
    "      - returns dict with keys: loss, logits\n",
    "      - logits shape: [B, 1]\n",
    "    Stability:\n",
    "      - pooling in fp32\n",
    "      - regression head run in fp32\n",
    "      - mse loss in fp32\n",
    "    \"\"\"\n",
    "    def __init__(self, base_model: nn.Module):\n",
    "        super().__init__()\n",
    "        self.base = base_model\n",
    "        hidden_size = getattr(base_model.config, \"hidden_size\", None)\n",
    "        if hidden_size is None:\n",
    "            hidden_size = getattr(getattr(base_model.config, \"text_config\", None), \"hidden_size\", None)\n",
    "        if hidden_size is None:\n",
    "            raise ValueError(\"Could not infer hidden_size from model config.\")\n",
    "        self.reg_head = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, labels=None, **inputs):\n",
    "        base_inputs = {k: v for k, v in inputs.items() if torch.is_tensor(v) and k != \"labels\"}\n",
    "\n",
    "        out = self.base(**base_inputs, output_hidden_states=False, return_dict=True)\n",
    "        last_hidden = out.last_hidden_state  # [B, T, H]\n",
    "\n",
    "        attn = base_inputs.get(\"attention_mask\", None)\n",
    "        if attn is None:\n",
    "            pooled = last_hidden.float().mean(dim=1)\n",
    "        else:\n",
    "            attn_f = attn.float()\n",
    "            lh_f = last_hidden.float()\n",
    "            denom = attn_f.sum(dim=1).clamp_min(1.0).unsqueeze(-1)\n",
    "            pooled = (lh_f * attn_f.unsqueeze(-1)).sum(dim=1) / denom\n",
    "\n",
    "        logits = self.reg_head(pooled).float()  # [B, 1] fp32\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = F.mse_loss(logits.squeeze(-1), labels.float())\n",
    "\n",
    "        return {\"loss\": loss, \"logits\": logits}\n",
    "\n",
    "model = Qwen2VLForRegression(base)\n",
    "\n",
    "print(\"[lora] Applying LoRA (FEATURE_EXTRACTION)...\")\n",
    "target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    "lora_cfg = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    target_modules=target_modules,\n",
    "    task_type=TaskType.FEATURE_EXTRACTION,\n",
    ")\n",
    "model.base = get_peft_model(model.base, lora_cfg)\n",
    "model.base.print_trainable_parameters()\n",
    "\n",
    "for p in model.reg_head.parameters():\n",
    "    p.requires_grad = True\n",
    "print(\"[model] reg_head params:\", sum(p.numel() for p in model.reg_head.parameters()))\n",
    "\n",
    "# ----------------------------\n",
    "# 6) Collator (truncation + token-range checks)\n",
    "# ----------------------------\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "class Qwen2VLRegressionCollator:\n",
    "    def __init__(self, processor, prompt_prefix: str, max_text_len: int, debug: bool = False):\n",
    "        self.processor = processor\n",
    "        self.prompt_prefix = prompt_prefix\n",
    "        self.max_text_len = max_text_len\n",
    "        self.debug = debug\n",
    "        self._seen = 0\n",
    "\n",
    "        self.vocab_size = None\n",
    "        tok = getattr(processor, \"tokenizer\", None)\n",
    "        if tok is not None and hasattr(tok, \"__len__\"):\n",
    "            self.vocab_size = int(len(tok))  # includes added/special tokens\n",
    "\n",
    "    def __call__(self, batch: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        images = [ex[\"image\"] for ex in batch]\n",
    "        captions = [ex[\"caption\"] for ex in batch]\n",
    "        labels = torch.tensor([float(ex[\"labels\"]) for ex in batch], dtype=torch.float32)\n",
    "\n",
    "        texts = []\n",
    "        for cap in captions:\n",
    "            prompt = f\"{self.prompt_prefix}\\n{cap}\\nScore:\"\n",
    "            messages = [{\"role\": \"user\", \"content\": [{\"type\": \"image\"}, {\"type\": \"text\", \"text\": prompt}]}]\n",
    "            text = self.processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "            texts.append(text)\n",
    "\n",
    "        model_inputs = self.processor(\n",
    "            text=texts,\n",
    "            images=images,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=self.max_text_len,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        model_inputs[\"labels\"] = labels\n",
    "\n",
    "        if \"input_ids\" in model_inputs:\n",
    "            max_id = int(model_inputs[\"input_ids\"].max())\n",
    "            if self.vocab_size is not None and max_id >= self.vocab_size:\n",
    "                raise ValueError(f\"input_ids has id {max_id} >= vocab_size {self.vocab_size}\")\n",
    "\n",
    "        if self.debug and self._seen < 3:\n",
    "            print(\"[collator] prompt preview:\\n\", texts[0][:400], \"...\")\n",
    "            print(\"[collator] input_ids:\", tuple(model_inputs[\"input_ids\"].shape))\n",
    "            print(\"[collator] labels:\", labels[: min(4, len(labels))].tolist())\n",
    "            self._seen += 1\n",
    "\n",
    "        return model_inputs\n",
    "\n",
    "collator = Qwen2VLRegressionCollator(processor, PROMPT_PREFIX, MAX_TEXT_LEN, debug=False)\n",
    "\n",
    "b = collator([train_ds[0]])\n",
    "print(\"[collator] keys:\", list(b.keys()))\n",
    "\n",
    "# ----------------------------\n",
    "# 7) Trainer + progress prints + resume\n",
    "# ----------------------------\n",
    "from transformers import TrainingArguments, Trainer, set_seed, TrainerCallback\n",
    "import numpy as np\n",
    "import time\n",
    "import re\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    preds, labels = eval_pred\n",
    "    preds = np.asarray(preds, dtype=np.float32).squeeze()   # handles [N,1] or [N]\n",
    "    labels = np.asarray(labels, dtype=np.float32).squeeze()\n",
    "\n",
    "    bad = ~np.isfinite(preds)\n",
    "    if bad.any():\n",
    "        return {\n",
    "            \"mse\": float(\"nan\"),\n",
    "            \"mae\": float(\"nan\"),\n",
    "            \"r2\": float(\"nan\"),\n",
    "            \"pred_nan_frac\": float(bad.mean()),\n",
    "        }\n",
    "\n",
    "    mse = float(np.mean((preds - labels) ** 2))\n",
    "    mae = float(np.mean(np.abs(preds - labels)))\n",
    "    var = float(np.var(labels))\n",
    "    r2 = float(\"nan\") if var == 0.0 else float(1.0 - mse / var)\n",
    "    return {\"mse\": mse, \"mae\": mae, \"r2\": r2}\n",
    "\n",
    "class PrintProgressCallback(TrainerCallback):\n",
    "    def __init__(self, trainer_ref=None, sample_every_steps: int = 100):\n",
    "        self.trainer_ref = trainer_ref\n",
    "        self.sample_every_steps = sample_every_steps\n",
    "        self.t0 = None\n",
    "\n",
    "    def on_train_begin(self, args, state, control, **kwargs):\n",
    "        self.t0 = time.time()\n",
    "        print(f\"[train] begin | max_steps={state.max_steps} | epochs={args.num_train_epochs}\")\n",
    "\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if not logs:\n",
    "            return\n",
    "        elapsed = (time.time() - self.t0) if self.t0 else 0.0\n",
    "        msg = f\"[log] step={state.global_step} epoch={state.epoch:.3f} elapsed={elapsed/60:.1f}m\"\n",
    "        if \"loss\" in logs:\n",
    "            msg += f\" loss={logs['loss']:.4f}\"\n",
    "        if \"learning_rate\" in logs:\n",
    "            msg += f\" lr={logs['learning_rate']:.2e}\"\n",
    "        if \"grad_norm\" in logs:\n",
    "            msg += f\" grad_norm={logs['grad_norm']:.3f}\"\n",
    "        print(msg)\n",
    "\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        if self.trainer_ref is None:\n",
    "            return\n",
    "        if state.global_step > 0 and (state.global_step % self.sample_every_steps == 0):\n",
    "            try:\n",
    "                m = self.trainer_ref.model\n",
    "                m.eval()\n",
    "                ex = train_ds[0]\n",
    "                batch = collator([ex])\n",
    "                device = next(m.parameters()).device\n",
    "                batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n",
    "                with torch.no_grad():\n",
    "                    out = m(**batch)\n",
    "                pred = float(out[\"logits\"].detach().float().cpu().squeeze().item())\n",
    "                print(f\"[sample] step={state.global_step} pred={pred:.4f} label={float(ex['labels']):.4f}\")\n",
    "            except Exception as e:\n",
    "                print(\"[sample] failed:\", repr(e))\n",
    "\n",
    "def get_latest_checkpoint(output_dir: str) -> Optional[str]:\n",
    "    out = Path(output_dir)\n",
    "    if not out.exists():\n",
    "        return None\n",
    "    ckpts = [p for p in out.glob(\"checkpoint-*\") if p.is_dir()]\n",
    "    if not ckpts:\n",
    "        return None\n",
    "\n",
    "    def step_num(p: Path) -> int:\n",
    "        m = re.search(r\"checkpoint-(\\d+)\", p.name)\n",
    "        return int(m.group(1)) if m else -1\n",
    "\n",
    "    return str(max(ckpts, key=step_num))\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=PER_DEVICE_BATCH_SIZE,\n",
    "    per_device_eval_batch_size=PER_DEVICE_BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRAD_ACCUM_STEPS,\n",
    "    learning_rate=LR,\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    logging_steps=10,\n",
    "\n",
    "    save_steps=200,\n",
    "    save_total_limit=2,\n",
    "\n",
    "    eval_strategy=\"steps\" if eval_ds is not None else \"no\",\n",
    "    eval_steps=200 if eval_ds is not None else None,\n",
    "\n",
    "    bf16=False,\n",
    "    fp16=True,\n",
    "    dataloader_num_workers=NUM_WORKERS,\n",
    "    remove_unused_columns=False,\n",
    "    max_steps=MAX_STEPS,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    data_collator=collator,\n",
    "    compute_metrics=compute_metrics if eval_ds is not None else None,\n",
    ")\n",
    "\n",
    "trainer.add_callback(PrintProgressCallback(trainer_ref=trainer, sample_every_steps=100))\n",
    "\n",
    "latest_ckpt = get_latest_checkpoint(OUTPUT_DIR)\n",
    "print(\"[resume] latest_ckpt:\", latest_ckpt)\n",
    "\n",
    "print(\"[train] Starting training...\")\n",
    "trainer.train(resume_from_checkpoint=latest_ckpt if latest_ckpt else None)\n",
    "print(\"[train] Training finished.\")\n",
    "\n",
    "# ----------------------------\n",
    "# 8) Save\n",
    "# ----------------------------\n",
    "print(\"[save] Saving full model + processor to:\", OUTPUT_DIR)\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "processor.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "lora_dir = str(Path(OUTPUT_DIR) / \"lora_adapter\")\n",
    "print(\"[save] Saving LoRA adapter to:\", lora_dir)\n",
    "model.base.save_pretrained(lora_dir)\n",
    "\n",
    "print(\"[save] Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02dc7c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pkill -9 -u $USER -f \"torchrun|accelerate|deepspeed\"\n",
    "!pkill -9 -u $USER -f \"python.*train|python.*ipykernel\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be32a5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba9bd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(\"cuda available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    free, total = torch.cuda.mem_get_info()\n",
    "    print(\"torch free GB:\", free/1024**3, \"total GB:\", total/1024**3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759d4d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load latest checkpoint + eval on val + return arrays (labels, preds)\n",
    "# ✅ prints progress during prediction/eval\n",
    "\n",
    "from pathlib import Path\n",
    "import os, re, math\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoProcessor, AutoModel, TrainingArguments, Trainer\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 0) Paths / config (edit if needed)\n",
    "# -----------------------------\n",
    "HF_CACHE_ROOT = \"/work/hdd/bfrc\"\n",
    "CACHE_DIR = str(Path(HF_CACHE_ROOT) / \"cache_dir\")\n",
    "\n",
    "EVAL_CSV  = \"\"\n",
    "IMG_ROOT  = \"\"\n",
    "MODEL_ID  = \"Qwen/Qwen2.5-VL-3B-Instruct\"\n",
    "\n",
    "OUTPUT_DIR = str(Path(HF_CACHE_ROOT) / \"finetunes\" / \"qwen2vl-regression-out\")\n",
    "\n",
    "PROMPT_PREFIX = \"Predict the alignment score (a real number). Caption:\"\n",
    "MAX_TEXT_LEN = 2048\n",
    "\n",
    "PER_DEVICE_EVAL_BATCH_SIZE = 4\n",
    "NUM_WORKERS = 2\n",
    "\n",
    "# LoRA (MUST match training)\n",
    "LORA_R = 16\n",
    "LORA_ALPHA = 32\n",
    "LORA_DROPOUT = 0.05\n",
    "TARGET_MODULES = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Find latest checkpoint\n",
    "# -----------------------------\n",
    "def get_latest_checkpoint(output_dir: str) -> Optional[str]:\n",
    "    out = Path(output_dir)\n",
    "    if not out.exists():\n",
    "        return None\n",
    "    ckpts = [p for p in out.glob(\"checkpoint-*\") if p.is_dir()]\n",
    "    if not ckpts:\n",
    "        return None\n",
    "\n",
    "    def step_num(p: Path) -> int:\n",
    "        m = re.search(r\"checkpoint-(\\d+)\", p.name)\n",
    "        return int(m.group(1)) if m else -1\n",
    "\n",
    "    return str(max(ckpts, key=step_num))\n",
    "\n",
    "ckpt_dir = get_latest_checkpoint(OUTPUT_DIR)\n",
    "if ckpt_dir is None:\n",
    "    raise FileNotFoundError(f\"No checkpoint-* found under: {OUTPUT_DIR}\")\n",
    "print(\"[ckpt] latest:\", ckpt_dir)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Val dataset\n",
    "# -----------------------------\n",
    "required_cols = {\"img_name\", \"caption\", \"alignment_score\"}\n",
    "raw = load_dataset(\"csv\", data_files={\"eval\": EVAL_CSV}, cache_dir=CACHE_DIR)\n",
    "missing = required_cols - set(raw[\"eval\"].column_names)\n",
    "if missing:\n",
    "    raise ValueError(f\"eval is missing columns: {missing}\")\n",
    "\n",
    "def load_image(path: str) -> Image.Image:\n",
    "    with Image.open(path) as im:\n",
    "        return im.convert(\"RGB\")\n",
    "\n",
    "class CSVImageCaptionRegressionDataset(Dataset):\n",
    "    def __init__(self, hf_dataset, img_root: str):\n",
    "        self.ds = hf_dataset\n",
    "        self.img_root = img_root\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.ds[idx]\n",
    "        img_path = os.path.join(self.img_root, str(row[\"img_name\"]))\n",
    "        caption = str(row[\"caption\"])\n",
    "        label = float(row[\"alignment_score\"])\n",
    "        if not math.isfinite(label):\n",
    "            raise ValueError(f\"Non-finite label at idx={idx}: {label}\")\n",
    "        image = load_image(img_path)\n",
    "        return {\"image\": image, \"caption\": caption, \"labels\": label, \"img_path\": img_path}\n",
    "\n",
    "eval_ds = CSVImageCaptionRegressionDataset(raw[\"eval\"], IMG_ROOT)\n",
    "print(\"[data] eval rows:\", len(eval_ds))\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Processor + model (must match training)\n",
    "# -----------------------------\n",
    "print(\"[model] Loading processor...\")\n",
    "processor = AutoProcessor.from_pretrained(MODEL_ID, trust_remote_code=True, cache_dir=CACHE_DIR)\n",
    "\n",
    "print(\"[model] Loading base...\")\n",
    "base = AutoModel.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    trust_remote_code=True,\n",
    "    cache_dir=CACHE_DIR,\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "\n",
    "class Qwen2VLForRegression(nn.Module):\n",
    "    def __init__(self, base_model: nn.Module):\n",
    "        super().__init__()\n",
    "        self.base = base_model\n",
    "        hidden_size = getattr(base_model.config, \"hidden_size\", None)\n",
    "        if hidden_size is None:\n",
    "            hidden_size = getattr(getattr(base_model.config, \"text_config\", None), \"hidden_size\", None)\n",
    "        if hidden_size is None:\n",
    "            raise ValueError(\"Could not infer hidden_size from model config.\")\n",
    "        self.reg_head = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, labels=None, **inputs):\n",
    "        base_inputs = {k: v for k, v in inputs.items() if torch.is_tensor(v) and k != \"labels\"}\n",
    "        out = self.base(**base_inputs, output_hidden_states=False, return_dict=True)\n",
    "        last_hidden = out.last_hidden_state  # [B,T,H]\n",
    "\n",
    "        attn = base_inputs.get(\"attention_mask\", None)\n",
    "        if attn is None:\n",
    "            pooled = last_hidden.float().mean(dim=1)\n",
    "        else:\n",
    "            attn_f = attn.float()\n",
    "            lh_f = last_hidden.float()\n",
    "            denom = attn_f.sum(dim=1).clamp_min(1.0).unsqueeze(-1)\n",
    "            pooled = (lh_f * attn_f.unsqueeze(-1)).sum(dim=1) / denom\n",
    "\n",
    "        logits = self.reg_head(pooled).float()  # [B,1]\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = F.mse_loss(logits.squeeze(-1), labels.float())\n",
    "        return {\"loss\": loss, \"logits\": logits}\n",
    "\n",
    "model = Qwen2VLForRegression(base)\n",
    "\n",
    "print(\"[lora] Applying LoRA...\")\n",
    "lora_cfg = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    target_modules=TARGET_MODULES,\n",
    "    task_type=TaskType.FEATURE_EXTRACTION,\n",
    ")\n",
    "model.base = get_peft_model(model.base, lora_cfg)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Collator\n",
    "# -----------------------------\n",
    "class Qwen2VLRegressionCollator:\n",
    "    def __init__(self, processor, prompt_prefix: str, max_text_len: int):\n",
    "        self.processor = processor\n",
    "        self.prompt_prefix = prompt_prefix\n",
    "        self.max_text_len = max_text_len\n",
    "\n",
    "    def __call__(self, batch: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        images = [ex[\"image\"] for ex in batch]\n",
    "        captions = [ex[\"caption\"] for ex in batch]\n",
    "        labels = torch.tensor([float(ex[\"labels\"]) for ex in batch], dtype=torch.float32)\n",
    "\n",
    "        texts = []\n",
    "        for cap in captions:\n",
    "            prompt = f\"{self.prompt_prefix}\\n{cap}\\nScore:\"\n",
    "            messages = [{\"role\": \"user\", \"content\": [{\"type\": \"image\"}, {\"type\": \"text\", \"text\": prompt}]}]\n",
    "            text = self.processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "            texts.append(text)\n",
    "\n",
    "        model_inputs = self.processor(\n",
    "            text=texts,\n",
    "            images=images,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=self.max_text_len,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        model_inputs[\"labels\"] = labels\n",
    "        return model_inputs\n",
    "\n",
    "collator = Qwen2VLRegressionCollator(processor, PROMPT_PREFIX, MAX_TEXT_LEN)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Load checkpoint weights\n",
    "# -----------------------------\n",
    "def load_checkpoint_state(model: nn.Module, ckpt_dir: str):\n",
    "    ckpt = Path(ckpt_dir)\n",
    "    st_path = ckpt / \"model.safetensors\"\n",
    "    pt_path = ckpt / \"pytorch_model.bin\"\n",
    "\n",
    "    if st_path.exists():\n",
    "        from safetensors.torch import load_file\n",
    "        print(\"[ckpt] loading safetensors:\", st_path)\n",
    "        state = load_file(str(st_path))\n",
    "    elif pt_path.exists():\n",
    "        print(\"[ckpt] loading pytorch bin:\", pt_path)\n",
    "        state = torch.load(str(pt_path), map_location=\"cpu\")\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"No model.safetensors or pytorch_model.bin in {ckpt_dir}\")\n",
    "\n",
    "    missing, unexpected = model.load_state_dict(state, strict=False)\n",
    "    print(\"[ckpt] missing keys:\", len(missing), \"| unexpected keys:\", len(unexpected))\n",
    "\n",
    "load_checkpoint_state(model, ckpt_dir)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 6) Evaluate + get arrays with progress printing\n",
    "# -----------------------------\n",
    "def compute_metrics(eval_pred):\n",
    "    preds, labels = eval_pred\n",
    "    preds = np.asarray(preds, dtype=np.float32).squeeze()\n",
    "    labels = np.asarray(labels, dtype=np.float32).squeeze()\n",
    "    mse = float(np.mean((preds - labels) ** 2))\n",
    "    mae = float(np.mean(np.abs(preds - labels)))\n",
    "    var = float(np.var(labels))\n",
    "    r2 = float(\"nan\") if var == 0.0 else float(1.0 - mse / var)\n",
    "    return {\"mse\": mse, \"mae\": mae, \"r2\": r2}\n",
    "\n",
    "eval_args = TrainingArguments(\n",
    "    output_dir=str(Path(OUTPUT_DIR) / \"_eval_tmp\"),\n",
    "    per_device_eval_batch_size=PER_DEVICE_EVAL_BATCH_SIZE,\n",
    "    dataloader_num_workers=NUM_WORKERS,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    bf16=False,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=eval_args,\n",
    "    eval_dataset=eval_ds,\n",
    "    data_collator=collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# ---- Progressy manual predict loop (prints batches) ----\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "loader = DataLoader(\n",
    "    eval_ds,\n",
    "    batch_size=PER_DEVICE_EVAL_BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    collate_fn=collator,\n",
    "    pin_memory=torch.cuda.is_available(),\n",
    ")\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "print(\"[predict] running manual loop with progress prints...\")\n",
    "total = len(loader)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(loader, start=1):\n",
    "        labels = batch[\"labels\"].cpu().numpy().astype(np.float32)\n",
    "        batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n",
    "\n",
    "        out = model(**batch)\n",
    "        logits = out[\"logits\"].detach().float().cpu().numpy().astype(np.float32).squeeze()\n",
    "\n",
    "        all_labels.append(labels)\n",
    "        all_preds.append(logits)\n",
    "\n",
    "        # progress print\n",
    "        if i == 1 or i % 10 == 0 or i == total:\n",
    "            done = i * loader.batch_size\n",
    "            done = min(done, len(eval_ds))\n",
    "            print(f\"[predict] batch {i}/{total} | seen {done}/{len(eval_ds)}\")\n",
    "\n",
    "val_labels = np.concatenate(all_labels, axis=0).squeeze()\n",
    "val_predictions = np.concatenate(all_preds, axis=0).squeeze()\n",
    "\n",
    "print(\"[predict] done. shapes:\", val_labels.shape, val_predictions.shape)\n",
    "\n",
    "# Compute metrics on the arrays (same as compute_metrics)\n",
    "mse = float(np.mean((val_predictions - val_labels) ** 2))\n",
    "mae = float(np.mean(np.abs(val_predictions - val_labels)))\n",
    "var = float(np.var(val_labels))\n",
    "r2 = float(\"nan\") if var == 0.0 else float(1.0 - mse / var)\n",
    "\n",
    "print(\"[metrics] mse:\", mse)\n",
    "print(\"[metrics] mae:\", mae)\n",
    "print(\"[metrics] r2 :\", r2)\n",
    "\n",
    "print(\"[sample] first 5 (label, pred):\")\n",
    "for j in range(min(5, len(val_labels))):\n",
    "    print(f\"  {j}: label={val_labels[j]:.4f} pred={val_predictions[j]:.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4886e7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[sample] first 5 (label, pred):\")\n",
    "for j in range(min(100, len(val_labels))):\n",
    "    print(f\"  {j}: label={val_labels[j]:.4f} pred={val_predictions[j]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f57e1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load latest checkpoint + run predictions on test_set_final.csv + save CSV with new column `regressor_pred`\n",
    "# ✅ prints progress during prediction/eval\n",
    "\n",
    "from pathlib import Path\n",
    "import os, re, math\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoProcessor, AutoModel, TrainingArguments, Trainer\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 0) Paths / config (edit if needed)\n",
    "# -----------------------------\n",
    "HF_CACHE_ROOT = \"/work/hdd/bfrc\"\n",
    "CACHE_DIR = str(Path(HF_CACHE_ROOT) / \"cache_dir\")\n",
    "\n",
    "# ⬇️ replaced val path with test_set_final.csv\n",
    "EVAL_CSV  = \n",
    "IMG_ROOT  = \n",
    "MODEL_ID  = \"Qwen/Qwen2.5-VL-3B-Instruct\"\n",
    "\n",
    "OUTPUT_DIR = str(Path(HF_CACHE_ROOT) / \"finetunes\" / \"qwen2vl-regression-out\")\n",
    "\n",
    "PROMPT_PREFIX = \"Predict the alignment score (a real number). Caption:\"\n",
    "MAX_TEXT_LEN = 2048\n",
    "\n",
    "PER_DEVICE_EVAL_BATCH_SIZE = 4\n",
    "NUM_WORKERS = 2\n",
    "\n",
    "# LoRA (MUST match training)\n",
    "LORA_R = 16\n",
    "LORA_ALPHA = 32\n",
    "LORA_DROPOUT = 0.05\n",
    "TARGET_MODULES = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Find latest checkpoint\n",
    "# -----------------------------\n",
    "def get_latest_checkpoint(output_dir: str) -> Optional[str]:\n",
    "    out = Path(output_dir)\n",
    "    if not out.exists():\n",
    "        return None\n",
    "    ckpts = [p for p in out.glob(\"checkpoint-*\") if p.is_dir()]\n",
    "    if not ckpts:\n",
    "        return None\n",
    "\n",
    "    def step_num(p: Path) -> int:\n",
    "        m = re.search(r\"checkpoint-(\\d+)\", p.name)\n",
    "        return int(m.group(1)) if m else -1\n",
    "\n",
    "    return str(max(ckpts, key=step_num))\n",
    "\n",
    "ckpt_dir = get_latest_checkpoint(OUTPUT_DIR)\n",
    "if ckpt_dir is None:\n",
    "    raise FileNotFoundError(f\"No checkpoint-* found under: {OUTPUT_DIR}\")\n",
    "print(\"[ckpt] latest:\", ckpt_dir)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Dataset\n",
    "# -----------------------------\n",
    "required_cols = {\"img_name\", \"caption\", \"alignment_score\"}\n",
    "raw = load_dataset(\"csv\", data_files={\"eval\": EVAL_CSV}, cache_dir=CACHE_DIR)\n",
    "missing = required_cols - set(raw[\"eval\"].column_names)\n",
    "if missing:\n",
    "    raise ValueError(f\"eval is missing columns: {missing}\")\n",
    "\n",
    "def load_image(path: str) -> Image.Image:\n",
    "    with Image.open(path) as im:\n",
    "        return im.convert(\"RGB\")\n",
    "\n",
    "class CSVImageCaptionRegressionDataset(Dataset):\n",
    "    def __init__(self, hf_dataset, img_root: str):\n",
    "        self.ds = hf_dataset\n",
    "        self.img_root = img_root\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.ds[idx]\n",
    "        img_path = os.path.join(self.img_root, str(row[\"img_name\"]))\n",
    "        caption = str(row[\"caption\"])\n",
    "        label = float(row[\"alignment_score\"])\n",
    "        if not math.isfinite(label):\n",
    "            raise ValueError(f\"Non-finite label at idx={idx}: {label}\")\n",
    "        image = load_image(img_path)\n",
    "        return {\"image\": image, \"caption\": caption, \"labels\": label, \"img_path\": img_path}\n",
    "\n",
    "eval_ds = CSVImageCaptionRegressionDataset(raw[\"eval\"], IMG_ROOT)\n",
    "print(\"[data] rows:\", len(eval_ds))\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Processor + model (must match training)\n",
    "# -----------------------------\n",
    "print(\"[model] Loading processor...\")\n",
    "processor = AutoProcessor.from_pretrained(MODEL_ID, trust_remote_code=True, cache_dir=CACHE_DIR)\n",
    "\n",
    "print(\"[model] Loading base...\")\n",
    "base = AutoModel.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    trust_remote_code=True,\n",
    "    cache_dir=CACHE_DIR,\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "\n",
    "class Qwen2VLForRegression(nn.Module):\n",
    "    def __init__(self, base_model: nn.Module):\n",
    "        super().__init__()\n",
    "        self.base = base_model\n",
    "        hidden_size = getattr(base_model.config, \"hidden_size\", None)\n",
    "        if hidden_size is None:\n",
    "            hidden_size = getattr(getattr(base_model.config, \"text_config\", None), \"hidden_size\", None)\n",
    "        if hidden_size is None:\n",
    "            raise ValueError(\"Could not infer hidden_size from model config.\")\n",
    "        self.reg_head = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, labels=None, **inputs):\n",
    "        base_inputs = {k: v for k, v in inputs.items() if torch.is_tensor(v) and k != \"labels\"}\n",
    "        out = self.base(**base_inputs, output_hidden_states=False, return_dict=True)\n",
    "        last_hidden = out.last_hidden_state  # [B,T,H]\n",
    "\n",
    "        attn = base_inputs.get(\"attention_mask\", None)\n",
    "        if attn is None:\n",
    "            pooled = last_hidden.float().mean(dim=1)\n",
    "        else:\n",
    "            attn_f = attn.float()\n",
    "            lh_f = last_hidden.float()\n",
    "            denom = attn_f.sum(dim=1).clamp_min(1.0).unsqueeze(-1)\n",
    "            pooled = (lh_f * attn_f.unsqueeze(-1)).sum(dim=1) / denom\n",
    "\n",
    "        logits = self.reg_head(pooled).float()  # [B,1]\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = F.mse_loss(logits.squeeze(-1), labels.float())\n",
    "        return {\"loss\": loss, \"logits\": logits}\n",
    "\n",
    "model = Qwen2VLForRegression(base)\n",
    "\n",
    "print(\"[lora] Applying LoRA...\")\n",
    "lora_cfg = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    target_modules=TARGET_MODULES,\n",
    "    task_type=TaskType.FEATURE_EXTRACTION,\n",
    ")\n",
    "model.base = get_peft_model(model.base, lora_cfg)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Collator\n",
    "# -----------------------------\n",
    "class Qwen2VLRegressionCollator:\n",
    "    def __init__(self, processor, prompt_prefix: str, max_text_len: int):\n",
    "        self.processor = processor\n",
    "        self.prompt_prefix = prompt_prefix\n",
    "        self.max_text_len = max_text_len\n",
    "\n",
    "    def __call__(self, batch: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        images = [ex[\"image\"] for ex in batch]\n",
    "        captions = [ex[\"caption\"] for ex in batch]\n",
    "        labels = torch.tensor([float(ex[\"labels\"]) for ex in batch], dtype=torch.float32)\n",
    "\n",
    "        texts = []\n",
    "        for cap in captions:\n",
    "            prompt = f\"{self.prompt_prefix}\\n{cap}\\nScore:\"\n",
    "            messages = [{\"role\": \"user\", \"content\": [{\"type\": \"image\"}, {\"type\": \"text\", \"text\": prompt}]}]\n",
    "            text = self.processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "            texts.append(text)\n",
    "\n",
    "        model_inputs = self.processor(\n",
    "            text=texts,\n",
    "            images=images,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=self.max_text_len,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        model_inputs[\"labels\"] = labels\n",
    "        return model_inputs\n",
    "\n",
    "collator = Qwen2VLRegressionCollator(processor, PROMPT_PREFIX, MAX_TEXT_LEN)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Load checkpoint weights (robust)\n",
    "# -----------------------------\n",
    "def load_checkpoint_state(model: nn.Module, ckpt_dir: str):\n",
    "    ckpt = Path(ckpt_dir)\n",
    "\n",
    "    candidates = [\n",
    "        ckpt / \"model.safetensors\",\n",
    "        ckpt / \"pytorch_model.bin\",\n",
    "        ckpt / \"adapter_model.safetensors\",\n",
    "        ckpt / \"adapter_model.bin\",\n",
    "    ]\n",
    "\n",
    "    found = None\n",
    "    for p in candidates:\n",
    "        if p.exists():\n",
    "            found = p\n",
    "            break\n",
    "    if found is None:\n",
    "        raise FileNotFoundError(\n",
    "            f\"No supported checkpoint file found in {ckpt_dir}. \"\n",
    "            f\"Tried: {[str(p.name) for p in candidates]}\"\n",
    "        )\n",
    "\n",
    "    if found.suffix == \".safetensors\":\n",
    "        from safetensors.torch import load_file\n",
    "        print(\"[ckpt] loading safetensors:\", found)\n",
    "        state = load_file(str(found))\n",
    "    else:\n",
    "        print(\"[ckpt] loading pytorch bin:\", found)\n",
    "        state = torch.load(str(found), map_location=\"cpu\")\n",
    "\n",
    "    # If this is an adapter-only checkpoint, loading into full model may miss reg_head.\n",
    "    # We still try to load into full model first.\n",
    "    missing, unexpected = model.load_state_dict(state, strict=False)\n",
    "    print(\"[ckpt] missing keys:\", len(missing), \"| unexpected keys:\", len(unexpected))\n",
    "\n",
    "load_checkpoint_state(model, ckpt_dir)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 6) Manual predict loop (with progress) + metrics\n",
    "# -----------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "loader = DataLoader(\n",
    "    eval_ds,\n",
    "    batch_size=PER_DEVICE_EVAL_BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    collate_fn=collator,\n",
    "    pin_memory=torch.cuda.is_available(),\n",
    ")\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "print(\"[predict] running manual loop with progress prints...\")\n",
    "total = len(loader)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(loader, start=1):\n",
    "        labels = batch[\"labels\"].cpu().numpy().astype(np.float32)\n",
    "        batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n",
    "\n",
    "        out = model(**batch)\n",
    "        logits = out[\"logits\"].detach().float().cpu().numpy().astype(np.float32).squeeze()\n",
    "\n",
    "        all_labels.append(labels)\n",
    "        all_preds.append(logits)\n",
    "\n",
    "        if i == 1 or i % 10 == 0 or i == total:\n",
    "            done = i * loader.batch_size\n",
    "            done = min(done, len(eval_ds))\n",
    "            print(f\"[predict] batch {i}/{total} | seen {done}/{len(eval_ds)}\")\n",
    "\n",
    "val_labels = np.concatenate(all_labels, axis=0).squeeze()\n",
    "val_predictions = np.concatenate(all_preds, axis=0).squeeze()\n",
    "\n",
    "print(\"[predict] done. shapes:\", val_labels.shape, val_predictions.shape)\n",
    "\n",
    "mse = float(np.mean((val_predictions - val_labels) ** 2))\n",
    "mae = float(np.mean(np.abs(val_predictions - val_labels)))\n",
    "var = float(np.var(val_labels))\n",
    "r2 = float(\"nan\") if var == 0.0 else float(1.0 - mse / var)\n",
    "\n",
    "print(\"[metrics] mse:\", mse)\n",
    "print(\"[metrics] mae:\", mae)\n",
    "print(\"[metrics] r2 :\", r2)\n",
    "\n",
    "print(\"[sample] first 5 (label, pred):\")\n",
    "for j in range(min(5, len(val_labels))):\n",
    "    print(f\"  {j}: label={val_labels[j]:.4f} pred={val_predictions[j]:.4f}\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 7) Append predictions to CSV + save (prints full absolute paths)\n",
    "# -----------------------------\n",
    "pred_col = \"regressor_pred\"\n",
    "\n",
    "in_csv_path = Path(EVAL_CSV).resolve()\n",
    "out_csv_path = in_csv_path.with_name(in_csv_path.stem + \"_with_preds\" + in_csv_path.suffix)\n",
    "\n",
    "print(f\"[csv] input : {in_csv_path}\")\n",
    "print(f\"[csv] output: {out_csv_path}\")\n",
    "\n",
    "df = pd.read_csv(in_csv_path)\n",
    "\n",
    "if len(df) != len(val_predictions):\n",
    "    raise ValueError(\n",
    "        f\"Row mismatch: CSV has {len(df)} rows but val_predictions has {len(val_predictions)}. \"\n",
    "        \"Ensure eval_ds order matches CSV order and shuffle=False.\"\n",
    "    )\n",
    "\n",
    "df[pred_col] = np.asarray(val_predictions, dtype=np.float32).reshape(-1)\n",
    "df.to_csv(out_csv_path, index=False)\n",
    "\n",
    "print(f\"[csv] wrote: {out_csv_path} (added column: {pred_col})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30547b2d",
   "metadata": {},
   "source": [
    "roco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e365e3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "root = Path(\"\")  # e.g., \"/mnt/data/dataset\" or r\"C:\\data\\dataset\"\n",
    "\n",
    "# pick the first folder (sorted for determinism)\n",
    "subfolders = sorted([p for p in root.iterdir() if p.is_dir()])\n",
    "if not subfolders:\n",
    "    raise FileNotFoundError(f\"No subfolders found in: {root}\")\n",
    "\n",
    "first_folder = subfolders[0]\n",
    "\n",
    "# pick the first image in that folder (common extensions)\n",
    "img_exts = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\", \".webp\"}\n",
    "images = sorted([p for p in first_folder.iterdir() if p.is_file() and p.suffix.lower() in img_exts])\n",
    "if not images:\n",
    "    raise FileNotFoundError(f\"No image files found in: {first_folder}\")\n",
    "\n",
    "img_path = images[0]\n",
    "img = Image.open(img_path)\n",
    "arr = np.array(img)\n",
    "\n",
    "print(\"First folder:\", first_folder)\n",
    "print(\"Image path:\", img_path)\n",
    "print(\"PIL mode:\", img.mode)          # e.g., RGB, L\n",
    "print(\"PIL size (W,H):\", img.size)    # (width, height)\n",
    "print(\"NumPy shape:\", arr.shape)      # (H, W, C) or (H, W)\n",
    "print(\"dtype:\", arr.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0fc6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "def resize_images_in_folder(\n",
    "    in_dir: str,\n",
    "    out_dir: str,\n",
    "    size=(256, 256),          # (width, height)\n",
    "    keep_aspect: bool = False,\n",
    "    pad_color=(0, 0, 0),      # used only if keep_aspect=True\n",
    "):\n",
    "    in_path = Path(in_dir)\n",
    "    out_path = Path(out_dir)\n",
    "    out_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    img_exts = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\", \".webp\"}\n",
    "    files = sorted([p for p in in_path.iterdir() if p.is_file() and p.suffix.lower() in img_exts])\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No images found in: {in_path}\")\n",
    "\n",
    "    target_w, target_h = size\n",
    "\n",
    "    for p in files:\n",
    "        with Image.open(p) as img:\n",
    "            img = img.convert(\"RGB\")  # consistent output; remove if you want to preserve mode\n",
    "\n",
    "            if keep_aspect:\n",
    "                # Fit within target while preserving aspect ratio, then pad\n",
    "                img.thumbnail((target_w, target_h), Image.Resampling.LANCZOS)\n",
    "                canvas = Image.new(\"RGB\", (target_w, target_h), pad_color)\n",
    "                x = (target_w - img.width) // 2\n",
    "                y = (target_h - img.height) // 2\n",
    "                canvas.paste(img, (x, y))\n",
    "                out_img = canvas\n",
    "            else:\n",
    "                # Direct resize (may distort aspect ratio)\n",
    "                out_img = img.resize((target_w, target_h), Image.Resampling.LANCZOS)\n",
    "\n",
    "            out_file = out_path / p.name\n",
    "            if out_file.suffix.lower() in {\".jpg\", \".jpeg\"}:\n",
    "                out_img.save(out_file, quality=95)\n",
    "            else:\n",
    "                out_img.save(out_file)\n",
    "\n",
    "    print(f\"Resized {len(files)} images from {in_path} -> {out_path} to {size}.\")\n",
    "\n",
    "# --------- EDIT THESE ----------\n",
    "input_folder = \"roco\"\n",
    "output_folder = \"roco2\"\n",
    "# ------------------------------\n",
    "\n",
    "resize_images_in_folder(input_folder, output_folder, size=(256, 256), keep_aspect=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02433e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load latest checkpoint + run predictions on test_set_final.csv + save CSV with new column `regressor_pred`\n",
    "# ✅ prints progress during prediction/eval\n",
    "\n",
    "from pathlib import Path\n",
    "import os, re, math\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoProcessor, AutoModel, TrainingArguments, Trainer\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 0) Paths / config (edit if needed)\n",
    "# -----------------------------\n",
    "HF_CACHE_ROOT = \"/work/hdd/bfrc\"\n",
    "CACHE_DIR = str(Path(HF_CACHE_ROOT) / \"cache_dir\")\n",
    "\n",
    "# ⬇️ replaced val path with test_set_final.csv\n",
    "EVAL_CSV  = \"ROCO.csv\"\n",
    "\n",
    "IMG_ROOT  = \"\n",
    "MODEL_ID  = \"Qwen/Qwen2.5-VL-3B-Instruct\"\n",
    "\n",
    "OUTPUT_DIR = str(Path(HF_CACHE_ROOT) / \"finetunes\" / \"qwen2vl-regression-out\")\n",
    "\n",
    "PROMPT_PREFIX = \"Predict the alignment score (a real number). Caption:\"\n",
    "MAX_TEXT_LEN = 2048\n",
    "\n",
    "PER_DEVICE_EVAL_BATCH_SIZE = 4\n",
    "NUM_WORKERS = 2\n",
    "\n",
    "# LoRA (MUST match training)\n",
    "LORA_R = 16\n",
    "LORA_ALPHA = 32\n",
    "LORA_DROPOUT = 0.05\n",
    "TARGET_MODULES = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Find latest checkpoint\n",
    "# -----------------------------\n",
    "def get_latest_checkpoint(output_dir: str) -> Optional[str]:\n",
    "    out = Path(output_dir)\n",
    "    if not out.exists():\n",
    "        return None\n",
    "    ckpts = [p for p in out.glob(\"checkpoint-*\") if p.is_dir()]\n",
    "    if not ckpts:\n",
    "        return None\n",
    "\n",
    "    def step_num(p: Path) -> int:\n",
    "        m = re.search(r\"checkpoint-(\\d+)\", p.name)\n",
    "        return int(m.group(1)) if m else -1\n",
    "\n",
    "    return str(max(ckpts, key=step_num))\n",
    "\n",
    "ckpt_dir = get_latest_checkpoint(OUTPUT_DIR)\n",
    "if ckpt_dir is None:\n",
    "    raise FileNotFoundError(f\"No checkpoint-* found under: {OUTPUT_DIR}\")\n",
    "print(\"[ckpt] latest:\", ckpt_dir)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Dataset\n",
    "# -----------------------------\n",
    "required_cols = {\"img_name\", \"caption\", \"alignment_score\"}\n",
    "raw = load_dataset(\"csv\", data_files={\"eval\": EVAL_CSV}, cache_dir=CACHE_DIR)\n",
    "missing = required_cols - set(raw[\"eval\"].column_names)\n",
    "if missing:\n",
    "    raise ValueError(f\"eval is missing columns: {missing}\")\n",
    "\n",
    "def load_image(path: str) -> Image.Image:\n",
    "    with Image.open(path) as im:\n",
    "        return im.convert(\"RGB\")\n",
    "\n",
    "class CSVImageCaptionRegressionDataset(Dataset):\n",
    "    def __init__(self, hf_dataset, img_root: str):\n",
    "        self.ds = hf_dataset\n",
    "        self.img_root = img_root\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.ds[idx]\n",
    "        img_path = os.path.join(self.img_root, str(row[\"img_name\"]))\n",
    "        caption = str(row[\"caption\"])\n",
    "        label = float(row[\"alignment_score\"])\n",
    "        if not math.isfinite(label):\n",
    "            raise ValueError(f\"Non-finite label at idx={idx}: {label}\")\n",
    "        image = load_image(img_path)\n",
    "        return {\"image\": image, \"caption\": caption, \"labels\": label, \"img_path\": img_path}\n",
    "\n",
    "eval_ds = CSVImageCaptionRegressionDataset(raw[\"eval\"], IMG_ROOT)\n",
    "print(\"[data] rows:\", len(eval_ds))\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Processor + model (must match training)\n",
    "# -----------------------------\n",
    "print(\"[model] Loading processor...\")\n",
    "processor = AutoProcessor.from_pretrained(MODEL_ID, trust_remote_code=True, cache_dir=CACHE_DIR)\n",
    "\n",
    "print(\"[model] Loading base...\")\n",
    "base = AutoModel.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    trust_remote_code=True,\n",
    "    cache_dir=CACHE_DIR,\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "\n",
    "class Qwen2VLForRegression(nn.Module):\n",
    "    def __init__(self, base_model: nn.Module):\n",
    "        super().__init__()\n",
    "        self.base = base_model\n",
    "        hidden_size = getattr(base_model.config, \"hidden_size\", None)\n",
    "        if hidden_size is None:\n",
    "            hidden_size = getattr(getattr(base_model.config, \"text_config\", None), \"hidden_size\", None)\n",
    "        if hidden_size is None:\n",
    "            raise ValueError(\"Could not infer hidden_size from model config.\")\n",
    "        self.reg_head = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, labels=None, **inputs):\n",
    "        base_inputs = {k: v for k, v in inputs.items() if torch.is_tensor(v) and k != \"labels\"}\n",
    "        out = self.base(**base_inputs, output_hidden_states=False, return_dict=True)\n",
    "        last_hidden = out.last_hidden_state  # [B,T,H]\n",
    "\n",
    "        attn = base_inputs.get(\"attention_mask\", None)\n",
    "        if attn is None:\n",
    "            pooled = last_hidden.float().mean(dim=1)\n",
    "        else:\n",
    "            attn_f = attn.float()\n",
    "            lh_f = last_hidden.float()\n",
    "            denom = attn_f.sum(dim=1).clamp_min(1.0).unsqueeze(-1)\n",
    "            pooled = (lh_f * attn_f.unsqueeze(-1)).sum(dim=1) / denom\n",
    "\n",
    "        logits = self.reg_head(pooled).float()  # [B,1]\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = F.mse_loss(logits.squeeze(-1), labels.float())\n",
    "        return {\"loss\": loss, \"logits\": logits}\n",
    "\n",
    "model = Qwen2VLForRegression(base)\n",
    "\n",
    "print(\"[lora] Applying LoRA...\")\n",
    "lora_cfg = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    target_modules=TARGET_MODULES,\n",
    "    task_type=TaskType.FEATURE_EXTRACTION,\n",
    ")\n",
    "model.base = get_peft_model(model.base, lora_cfg)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Collator\n",
    "# -----------------------------\n",
    "class Qwen2VLRegressionCollator:\n",
    "    def __init__(self, processor, prompt_prefix: str, max_text_len: int):\n",
    "        self.processor = processor\n",
    "        self.prompt_prefix = prompt_prefix\n",
    "        self.max_text_len = max_text_len\n",
    "\n",
    "    def __call__(self, batch: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        images = [ex[\"image\"] for ex in batch]\n",
    "        captions = [ex[\"caption\"] for ex in batch]\n",
    "        labels = torch.tensor([float(ex[\"labels\"]) for ex in batch], dtype=torch.float32)\n",
    "\n",
    "        texts = []\n",
    "        for cap in captions:\n",
    "            prompt = f\"{self.prompt_prefix}\\n{cap}\\nScore:\"\n",
    "            messages = [{\"role\": \"user\", \"content\": [{\"type\": \"image\"}, {\"type\": \"text\", \"text\": prompt}]}]\n",
    "            text = self.processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "            texts.append(text)\n",
    "\n",
    "        model_inputs = self.processor(\n",
    "            text=texts,\n",
    "            images=images,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=self.max_text_len,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        model_inputs[\"labels\"] = labels\n",
    "        return model_inputs\n",
    "\n",
    "collator = Qwen2VLRegressionCollator(processor, PROMPT_PREFIX, MAX_TEXT_LEN)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Load checkpoint weights (robust)\n",
    "# -----------------------------\n",
    "def load_checkpoint_state(model: nn.Module, ckpt_dir: str):\n",
    "    ckpt = Path(ckpt_dir)\n",
    "\n",
    "    candidates = [\n",
    "        ckpt / \"model.safetensors\",\n",
    "        ckpt / \"pytorch_model.bin\",\n",
    "        ckpt / \"adapter_model.safetensors\",\n",
    "        ckpt / \"adapter_model.bin\",\n",
    "    ]\n",
    "\n",
    "    found = None\n",
    "    for p in candidates:\n",
    "        if p.exists():\n",
    "            found = p\n",
    "            break\n",
    "    if found is None:\n",
    "        raise FileNotFoundError(\n",
    "            f\"No supported checkpoint file found in {ckpt_dir}. \"\n",
    "            f\"Tried: {[str(p.name) for p in candidates]}\"\n",
    "        )\n",
    "\n",
    "    if found.suffix == \".safetensors\":\n",
    "        from safetensors.torch import load_file\n",
    "        print(\"[ckpt] loading safetensors:\", found)\n",
    "        state = load_file(str(found))\n",
    "    else:\n",
    "        print(\"[ckpt] loading pytorch bin:\", found)\n",
    "        state = torch.load(str(found), map_location=\"cpu\")\n",
    "\n",
    "    # If this is an adapter-only checkpoint, loading into full model may miss reg_head.\n",
    "    # We still try to load into full model first.\n",
    "    missing, unexpected = model.load_state_dict(state, strict=False)\n",
    "    print(\"[ckpt] missing keys:\", len(missing), \"| unexpected keys:\", len(unexpected))\n",
    "\n",
    "load_checkpoint_state(model, ckpt_dir)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 6) Manual predict loop (with progress) + metrics\n",
    "# -----------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "loader = DataLoader(\n",
    "    eval_ds,\n",
    "    batch_size=PER_DEVICE_EVAL_BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    collate_fn=collator,\n",
    "    pin_memory=torch.cuda.is_available(),\n",
    ")\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "print(\"[predict] running manual loop with progress prints...\")\n",
    "total = len(loader)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(loader, start=1):\n",
    "        labels = batch[\"labels\"].cpu().numpy().astype(np.float32)\n",
    "        batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n",
    "\n",
    "        out = model(**batch)\n",
    "        logits = out[\"logits\"].detach().float().cpu().numpy().astype(np.float32).squeeze()\n",
    "\n",
    "        all_labels.append(labels)\n",
    "        all_preds.append(logits)\n",
    "\n",
    "        if i == 1 or i % 10 == 0 or i == total:\n",
    "            done = i * loader.batch_size\n",
    "            done = min(done, len(eval_ds))\n",
    "            print(f\"[predict] batch {i}/{total} | seen {done}/{len(eval_ds)}\")\n",
    "\n",
    "val_labels = np.concatenate(all_labels, axis=0).squeeze()\n",
    "val_predictions = np.concatenate(all_preds, axis=0).squeeze()\n",
    "\n",
    "print(\"[predict] done. shapes:\", val_labels.shape, val_predictions.shape)\n",
    "\n",
    "mse = float(np.mean((val_predictions - val_labels) ** 2))\n",
    "mae = float(np.mean(np.abs(val_predictions - val_labels)))\n",
    "var = float(np.var(val_labels))\n",
    "r2 = float(\"nan\") if var == 0.0 else float(1.0 - mse / var)\n",
    "\n",
    "print(\"[metrics] mse:\", mse)\n",
    "print(\"[metrics] mae:\", mae)\n",
    "print(\"[metrics] r2 :\", r2)\n",
    "\n",
    "print(\"[sample] first 5 (label, pred):\")\n",
    "for j in range(min(5, len(val_labels))):\n",
    "    print(f\"  {j}: label={val_labels[j]:.4f} pred={val_predictions[j]:.4f}\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 7) Append predictions to CSV + save (prints full absolute paths)\n",
    "# -----------------------------\n",
    "pred_col = \"regressor_pred\"\n",
    "\n",
    "in_csv_path = Path(EVAL_CSV).resolve()\n",
    "out_csv_path = in_csv_path.with_name(in_csv_path.stem + \"_with_preds\" + in_csv_path.suffix)\n",
    "\n",
    "print(f\"[csv] input : {in_csv_path}\")\n",
    "print(f\"[csv] output: {out_csv_path}\")\n",
    "\n",
    "df = pd.read_csv(in_csv_path)\n",
    "\n",
    "if len(df) != len(val_predictions):\n",
    "    raise ValueError(\n",
    "        f\"Row mismatch: CSV has {len(df)} rows but val_predictions has {len(val_predictions)}. \"\n",
    "        \"Ensure eval_ds order matches CSV order and shuffle=False.\"\n",
    "    )\n",
    "\n",
    "df[pred_col] = np.asarray(val_predictions, dtype=np.float32).reshape(-1)\n",
    "df.to_csv(out_csv_path, index=False)\n",
    "\n",
    "print(f\"[csv] wrote: {out_csv_path} (added column: {pred_col})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4384d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qwen2.5-VL 3B regression fine-tuning (image + caption -> alignment_score)\n",
    "# ✅ LoRA (FEATURE_EXTRACTION)\n",
    "# ✅ Single-process / single-GPU\n",
    "# ✅ Resume from latest checkpoint automatically\n",
    "# ✅ Validation during training (every eval_steps)\n",
    "# ✅ Stability fixes: fp32 pooling + fp32 head + fp32 MSE loss\n",
    "# ✅ Trainer-compatible outputs: returns \"logits\" (not \"predictions\")\n",
    "#\n",
    "# CHANGES MADE (hyperparams only):\n",
    "# - LR: 2e-4 -> 5e-5\n",
    "# - WARMUP_RATIO: 0.03 -> 0.10\n",
    "# - WEIGHT_DECAY: 0.0 -> 0.01\n",
    "# - MAX_TEXT_LEN: 2048 -> 1024\n",
    "# - LORA_DROPOUT: 0.05 -> 0.10\n",
    "# - NUM_WORKERS: 2 -> 0\n",
    "# - TrainingArguments: max_grad_norm=1.0, lr_scheduler_type=\"cosine\", optim=\"adamw_torch_fused\"\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# ----------------------------\n",
    "# 1) IMPORTANT: env + cache\n",
    "# ----------------------------\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "# Optional: helps avoid tokenizer fork warning / deadlocks when using multiprocessing\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "HF_CACHE_ROOT = \"/work/hdd/bfrc\"\n",
    "os.environ[\"HF_HOME\"] = str(Path(HF_CACHE_ROOT) / \"hf_home\")\n",
    "os.environ[\"HUGGINGFACE_HUB_CACHE\"] = str(Path(HF_CACHE_ROOT) / \"hub\")\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = str(Path(HF_CACHE_ROOT) / \"transformers\")\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = str(Path(HF_CACHE_ROOT) / \"datasets\")\n",
    "CACHE_DIR = str(Path(HF_CACHE_ROOT) / \"cache_dir\")\n",
    "\n",
    "for p in [\n",
    "    os.environ[\"HF_HOME\"],\n",
    "    os.environ[\"HUGGINGFACE_HUB_CACHE\"],\n",
    "    os.environ[\"TRANSFORMERS_CACHE\"],\n",
    "    os.environ[\"HF_DATASETS_CACHE\"],\n",
    "    CACHE_DIR,\n",
    "]:\n",
    "    Path(p).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"[env] CUDA_LAUNCH_BLOCKING:\", os.environ[\"CUDA_LAUNCH_BLOCKING\"])\n",
    "print(\"[env] TOKENIZERS_PARALLELISM:\", os.environ.get(\"TOKENIZERS_PARALLELISM\"))\n",
    "print(\"[cache] HF_HOME:\", os.environ[\"HF_HOME\"])\n",
    "print(\"[cache] HUGGINGFACE_HUB_CACHE:\", os.environ[\"HUGGINGFACE_HUB_CACHE\"])\n",
    "print(\"[cache] TRANSFORMERS_CACHE:\", os.environ[\"TRANSFORMERS_CACHE\"])\n",
    "print(\"[cache] HF_DATASETS_CACHE:\", os.environ[\"HF_DATASETS_CACHE\"])\n",
    "print(\"[cache] CACHE_DIR:\", CACHE_DIR)\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Config\n",
    "# ----------------------------\n",
    "import torch\n",
    "\n",
    "TRAIN_CSV = \"/\n",
    "EVAL_CSV  = \"/\n",
    "IMG_ROOT  = \"\"\n",
    "\n",
    "MODEL_ID = \"Qwen/Qwen2.5-VL-3B-Instruct\"\n",
    "\n",
    "OUTPUT_DIR = str(Path(HF_CACHE_ROOT) / \"finetunes\" / \"qwen2vl-regression-out\")\n",
    "Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# training (UPDATED)\n",
    "EPOCHS = 2\n",
    "PER_DEVICE_BATCH_SIZE = 4\n",
    "GRAD_ACCUM_STEPS = 8\n",
    "LR = 5e-5                 # was 2e-4\n",
    "WARMUP_RATIO = 0.10       # was 0.03\n",
    "WEIGHT_DECAY = 0.01       # was 0.0\n",
    "MAX_STEPS = -1\n",
    "\n",
    "# prompt + truncation (UPDATED)\n",
    "PROMPT_PREFIX = \"Predict the alignment score (a real number). Caption:\"\n",
    "MAX_TEXT_LEN = 2048       # was 2048\n",
    "\n",
    "# LoRA (UPDATED)\n",
    "LORA_R = 16\n",
    "LORA_ALPHA = 32\n",
    "LORA_DROPOUT = 0.10       # was 0.05\n",
    "\n",
    "SEED = 42\n",
    "NUM_WORKERS = 0           # was 2 (avoid tokenizers fork issues)\n",
    "\n",
    "print(\"[cfg] OUTPUT_DIR:\", OUTPUT_DIR)\n",
    "print(\"[cfg] CUDA:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"[cfg] GPU:\", torch.cuda.get_device_name(0))\n",
    "    print(\"[cfg] device_count:\", torch.cuda.device_count())\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Load dataset\n",
    "# ----------------------------\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"[data] Loading CSV(s)...\")\n",
    "data_files = {\"train\": TRAIN_CSV}\n",
    "if EVAL_CSV:\n",
    "    data_files[\"eval\"] = EVAL_CSV\n",
    "\n",
    "raw = load_dataset(\"csv\", data_files=data_files, cache_dir=CACHE_DIR)\n",
    "\n",
    "print(\"[data] splits:\", list(raw.keys()))\n",
    "print(\"[data] columns:\", raw[\"train\"].column_names)\n",
    "print(\"[data] train rows:\", len(raw[\"train\"]))\n",
    "if EVAL_CSV:\n",
    "    print(\"[data] eval rows:\", len(raw[\"eval\"]))\n",
    "\n",
    "required_cols = {\"img_name\", \"caption\", \"alignment_score\"}\n",
    "for split in raw.keys():\n",
    "    missing = required_cols - set(raw[split].column_names)\n",
    "    if missing:\n",
    "        raise ValueError(f\"{split} is missing columns: {missing}\")\n",
    "\n",
    "print(\"[data] example row:\", raw[\"train\"][0])\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Torch dataset (image load on the fly)\n",
    "# ----------------------------\n",
    "import math\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "def load_image(path: str) -> Image.Image:\n",
    "    with Image.open(path) as im:\n",
    "        return im.convert(\"RGB\")\n",
    "\n",
    "class CSVImageCaptionRegressionDataset(Dataset):\n",
    "    def __init__(self, hf_dataset, img_root: str):\n",
    "        self.ds = hf_dataset\n",
    "        self.img_root = img_root\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.ds[idx]\n",
    "        img_path = os.path.join(self.img_root, str(row[\"img_name\"]))\n",
    "        caption = str(row[\"caption\"])\n",
    "        label = float(row[\"alignment_score\"])\n",
    "\n",
    "        if not math.isfinite(label):\n",
    "            raise ValueError(f\"Non-finite label at idx={idx}: {label}\")\n",
    "\n",
    "        image = load_image(img_path)\n",
    "        return {\"image\": image, \"caption\": caption, \"labels\": label, \"img_path\": img_path}\n",
    "\n",
    "train_ds = CSVImageCaptionRegressionDataset(raw[\"train\"], IMG_ROOT)\n",
    "eval_ds  = CSVImageCaptionRegressionDataset(raw[\"eval\"], IMG_ROOT) if EVAL_CSV else None\n",
    "print(\"[ds] train:\", len(train_ds), \"| eval:\", (len(eval_ds) if eval_ds else 0))\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Load processor + base model + regression head + LoRA\n",
    "# ----------------------------\n",
    "from transformers import AutoProcessor, AutoModel\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "print(\"[model] Loading processor...\")\n",
    "processor = AutoProcessor.from_pretrained(MODEL_ID, trust_remote_code=True, cache_dir=CACHE_DIR)\n",
    "\n",
    "print(\"[model] Loading base model (fp16)...\")\n",
    "base = AutoModel.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    trust_remote_code=True,\n",
    "    cache_dir=CACHE_DIR,\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "print(\"[model] Base loaded.\")\n",
    "\n",
    "class Qwen2VLForRegression(nn.Module):\n",
    "    \"\"\"\n",
    "    Trainer-friendly regression wrapper:\n",
    "      - returns dict with keys: loss, logits\n",
    "      - logits shape: [B, 1]\n",
    "    Stability:\n",
    "      - pooling in fp32\n",
    "      - regression head run in fp32\n",
    "      - mse loss in fp32\n",
    "    \"\"\"\n",
    "    def __init__(self, base_model: nn.Module):\n",
    "        super().__init__()\n",
    "        self.base = base_model\n",
    "        hidden_size = getattr(base_model.config, \"hidden_size\", None)\n",
    "        if hidden_size is None:\n",
    "            hidden_size = getattr(getattr(base_model.config, \"text_config\", None), \"hidden_size\", None)\n",
    "        if hidden_size is None:\n",
    "            raise ValueError(\"Could not infer hidden_size from model config.\")\n",
    "        self.reg_head = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, labels=None, **inputs):\n",
    "        base_inputs = {k: v for k, v in inputs.items() if torch.is_tensor(v) and k != \"labels\"}\n",
    "\n",
    "        out = self.base(**base_inputs, output_hidden_states=False, return_dict=True)\n",
    "        last_hidden = out.last_hidden_state  # [B, T, H]\n",
    "\n",
    "        attn = base_inputs.get(\"attention_mask\", None)\n",
    "        if attn is None:\n",
    "            pooled = last_hidden.float().mean(dim=1)\n",
    "        else:\n",
    "            attn_f = attn.float()\n",
    "            lh_f = last_hidden.float()\n",
    "            denom = attn_f.sum(dim=1).clamp_min(1.0).unsqueeze(-1)\n",
    "            pooled = (lh_f * attn_f.unsqueeze(-1)).sum(dim=1) / denom\n",
    "\n",
    "        logits = self.reg_head(pooled).float()  # [B, 1] fp32\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = F.mse_loss(logits.squeeze(-1), labels.float())\n",
    "\n",
    "        return {\"loss\": loss, \"logits\": logits}\n",
    "\n",
    "model = Qwen2VLForRegression(base)\n",
    "\n",
    "print(\"[lora] Applying LoRA (FEATURE_EXTRACTION)...\")\n",
    "target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    "lora_cfg = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    target_modules=target_modules,\n",
    "    task_type=TaskType.FEATURE_EXTRACTION,\n",
    ")\n",
    "model.base = get_peft_model(model.base, lora_cfg)\n",
    "model.base.print_trainable_parameters()\n",
    "\n",
    "for p in model.reg_head.parameters():\n",
    "    p.requires_grad = True\n",
    "print(\"[model] reg_head params:\", sum(p.numel() for p in model.reg_head.parameters()))\n",
    "\n",
    "# ----------------------------\n",
    "# 6) Collator (truncation + token-range checks)\n",
    "# ----------------------------\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "class Qwen2VLRegressionCollator:\n",
    "    def __init__(self, processor, prompt_prefix: str, max_text_len: int, debug: bool = False):\n",
    "        self.processor = processor\n",
    "        self.prompt_prefix = prompt_prefix\n",
    "        self.max_text_len = max_text_len\n",
    "        self.debug = debug\n",
    "        self._seen = 0\n",
    "\n",
    "        self.vocab_size = None\n",
    "        tok = getattr(processor, \"tokenizer\", None)\n",
    "        if tok is not None and hasattr(tok, \"__len__\"):\n",
    "            self.vocab_size = int(len(tok))  # includes added/special tokens\n",
    "\n",
    "    def __call__(self, batch: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        images = [ex[\"image\"] for ex in batch]\n",
    "        captions = [ex[\"caption\"] for ex in batch]\n",
    "        labels = torch.tensor([float(ex[\"labels\"]) for ex in batch], dtype=torch.float32)\n",
    "\n",
    "        texts = []\n",
    "        for cap in captions:\n",
    "            prompt = f\"{self.prompt_prefix}\\n{cap}\\nScore:\"\n",
    "            messages = [{\"role\": \"user\", \"content\": [{\"type\": \"image\"}, {\"type\": \"text\", \"text\": prompt}]}]\n",
    "            text = self.processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "            texts.append(text)\n",
    "\n",
    "        model_inputs = self.processor(\n",
    "            text=texts,\n",
    "            images=images,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=self.max_text_len,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        model_inputs[\"labels\"] = labels\n",
    "\n",
    "        if \"input_ids\" in model_inputs:\n",
    "            max_id = int(model_inputs[\"input_ids\"].max())\n",
    "            if self.vocab_size is not None and max_id >= self.vocab_size:\n",
    "                raise ValueError(f\"input_ids has id {max_id} >= vocab_size {self.vocab_size}\")\n",
    "\n",
    "        if self.debug and self._seen < 3:\n",
    "            print(\"[collator] prompt preview:\\n\", texts[0][:400], \"...\")\n",
    "            print(\"[collator] input_ids:\", tuple(model_inputs[\"input_ids\"].shape))\n",
    "            print(\"[collator] labels:\", labels[: min(4, len(labels))].tolist())\n",
    "            self._seen += 1\n",
    "\n",
    "        return model_inputs\n",
    "\n",
    "collator = Qwen2VLRegressionCollator(processor, PROMPT_PREFIX, MAX_TEXT_LEN, debug=False)\n",
    "\n",
    "b = collator([train_ds[0]])\n",
    "print(\"[collator] keys:\", list(b.keys()))\n",
    "\n",
    "# ----------------------------\n",
    "# 7) Trainer + progress prints + resume\n",
    "# ----------------------------\n",
    "from transformers import TrainingArguments, Trainer, set_seed, TrainerCallback\n",
    "import numpy as np\n",
    "import time\n",
    "import re\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    preds, labels = eval_pred\n",
    "    preds = np.asarray(preds, dtype=np.float32).squeeze()   # handles [N,1] or [N]\n",
    "    labels = np.asarray(labels, dtype=np.float32).squeeze()\n",
    "\n",
    "    bad = ~np.isfinite(preds)\n",
    "    if bad.any():\n",
    "        return {\n",
    "            \"mse\": float(\"nan\"),\n",
    "            \"mae\": float(\"nan\"),\n",
    "            \"r2\": float(\"nan\"),\n",
    "            \"pred_nan_frac\": float(bad.mean()),\n",
    "        }\n",
    "\n",
    "    mse = float(np.mean((preds - labels) ** 2))\n",
    "    mae = float(np.mean(np.abs(preds - labels)))\n",
    "    var = float(np.var(labels))\n",
    "    r2 = float(\"nan\") if var == 0.0 else float(1.0 - mse / var)\n",
    "    return {\"mse\": mse, \"mae\": mae, \"r2\": r2}\n",
    "\n",
    "class PrintProgressCallback(TrainerCallback):\n",
    "    def __init__(self, trainer_ref=None, sample_every_steps: int = 100):\n",
    "        self.trainer_ref = trainer_ref\n",
    "        self.sample_every_steps = sample_every_steps\n",
    "        self.t0 = None\n",
    "\n",
    "    def on_train_begin(self, args, state, control, **kwargs):\n",
    "        self.t0 = time.time()\n",
    "        print(f\"[train] begin | max_steps={state.max_steps} | epochs={args.num_train_epochs}\")\n",
    "\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if not logs:\n",
    "            return\n",
    "        elapsed = (time.time() - self.t0) if self.t0 else 0.0\n",
    "        msg = f\"[log] step={state.global_step} epoch={state.epoch:.3f} elapsed={elapsed/60:.1f}m\"\n",
    "        if \"loss\" in logs:\n",
    "            msg += f\" loss={logs['loss']:.4f}\"\n",
    "        if \"learning_rate\" in logs:\n",
    "            msg += f\" lr={logs['learning_rate']:.2e}\"\n",
    "        if \"grad_norm\" in logs:\n",
    "            msg += f\" grad_norm={logs['grad_norm']:.3f}\"\n",
    "        print(msg)\n",
    "\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        if self.trainer_ref is None:\n",
    "            return\n",
    "        if state.global_step > 0 and (state.global_step % self.sample_every_steps == 0):\n",
    "            try:\n",
    "                m = self.trainer_ref.model\n",
    "                m.eval()\n",
    "                ex = train_ds[0]\n",
    "                batch = collator([ex])\n",
    "                device = next(m.parameters()).device\n",
    "                batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n",
    "                with torch.no_grad():\n",
    "                    out = m(**batch)\n",
    "                pred = float(out[\"logits\"].detach().float().cpu().squeeze().item())\n",
    "                print(f\"[sample] step={state.global_step} pred={pred:.4f} label={float(ex['labels']):.4f}\")\n",
    "            except Exception as e:\n",
    "                print(\"[sample] failed:\", repr(e))\n",
    "\n",
    "def get_latest_checkpoint(output_dir: str) -> Optional[str]:\n",
    "    out = Path(output_dir)\n",
    "    if not out.exists():\n",
    "        return None\n",
    "    ckpts = [p for p in out.glob(\"checkpoint-*\") if p.is_dir()]\n",
    "    if not ckpts:\n",
    "        return None\n",
    "\n",
    "    def step_num(p: Path) -> int:\n",
    "        m = re.search(r\"checkpoint-(\\d+)\", p.name)\n",
    "        return int(m.group(1)) if m else -1\n",
    "\n",
    "    return str(max(ckpts, key=step_num))\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=PER_DEVICE_BATCH_SIZE,\n",
    "    per_device_eval_batch_size=PER_DEVICE_BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRAD_ACCUM_STEPS,\n",
    "    learning_rate=LR,\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    logging_steps=10,\n",
    "\n",
    "    save_steps=200,\n",
    "    save_total_limit=2,\n",
    "\n",
    "    eval_strategy=\"steps\" if eval_ds is not None else \"no\",\n",
    "    eval_steps=200 if eval_ds is not None else None,\n",
    "\n",
    "    bf16=False,\n",
    "    fp16=True,\n",
    "\n",
    "    dataloader_num_workers=NUM_WORKERS,\n",
    "    remove_unused_columns=False,\n",
    "    max_steps=MAX_STEPS,\n",
    "    report_to=\"none\",\n",
    "\n",
    "    # UPDATED stability knobs\n",
    "    max_grad_norm=1.0,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    optim=\"adamw_torch_fused\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    data_collator=collator,\n",
    "    compute_metrics=compute_metrics if eval_ds is not None else None,\n",
    ")\n",
    "\n",
    "trainer.add_callback(PrintProgressCallback(trainer_ref=trainer, sample_every_steps=100))\n",
    "\n",
    "latest_ckpt = get_latest_checkpoint(OUTPUT_DIR)\n",
    "print(\"[resume] latest_ckpt:\", latest_ckpt)\n",
    "\n",
    "print(\"[train] Starting training...\")\n",
    "trainer.train(resume_from_checkpoint=latest_ckpt if latest_ckpt else None)\n",
    "print(\"[train] Training finished.\")\n",
    "\n",
    "# ----------------------------\n",
    "# 8) Save\n",
    "# ----------------------------\n",
    "print(\"[save] Saving full model + processor to:\", OUTPUT_DIR)\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "processor.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "lora_dir = str(Path(OUTPUT_DIR) / \"lora_adapter\")\n",
    "print(\"[save] Saving LoRA adapter to:\", lora_dir)\n",
    "model.base.save_pretrained(lora_dir)\n",
    "\n",
    "print(\"[save] Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1714c49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qwen2.5-VL 3B regression fine-tuning (image + caption -> alignment_score)\n",
    "# ✅ LoRA (FEATURE_EXTRACTION)\n",
    "# ✅ Single-process / single-GPU\n",
    "# ✅ Resume from latest checkpoint automatically\n",
    "# ✅ Validation during training (every eval_steps)\n",
    "# ✅ Stability fixes: fp32 pooling + fp32 head + fp32 MSE loss\n",
    "# ✅ Trainer-compatible outputs: returns \"logits\" (not \"predictions\")\n",
    "#\n",
    "# MODS (hyperparams only, plus printing sanity):\n",
    "# - Keep MAX_TEXT_LEN=2048 (avoid Qwen2.5-VL image-token truncation mismatch)\n",
    "# - LR: 2e-4 -> 5e-5\n",
    "# - WARMUP_RATIO: 0.03 -> 0.10\n",
    "# - WEIGHT_DECAY: 0.0 -> 0.01\n",
    "# - LORA_DROPOUT: 0.05 -> 0.10\n",
    "# - NUM_WORKERS: 2 -> 0\n",
    "# - TrainingArguments: max_grad_norm=1.0, lr_scheduler_type=\"cosine\"\n",
    "# - optim: try \"adamw_torch_fused\" (falls back to \"adamw_torch\" if unavailable)\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# ----------------------------\n",
    "# 1) IMPORTANT: env + cache\n",
    "# ----------------------------\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "HF_CACHE_ROOT = \"/work/hdd/bfrc\"\n",
    "os.environ[\"HF_HOME\"] = str(Path(HF_CACHE_ROOT) / \"hf_home\")\n",
    "os.environ[\"HUGGINGFACE_HUB_CACHE\"] = str(Path(HF_CACHE_ROOT) / \"hub\")\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = str(Path(HF_CACHE_ROOT) / \"transformers\")\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = str(Path(HF_CACHE_ROOT) / \"datasets\")\n",
    "CACHE_DIR = str(Path(HF_CACHE_ROOT) / \"cache_dir\")\n",
    "\n",
    "for p in [\n",
    "    os.environ[\"HF_HOME\"],\n",
    "    os.environ[\"HUGGINGFACE_HUB_CACHE\"],\n",
    "    os.environ[\"TRANSFORMERS_CACHE\"],\n",
    "    os.environ[\"HF_DATASETS_CACHE\"],\n",
    "    CACHE_DIR,\n",
    "]:\n",
    "    Path(p).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"[env] CUDA_LAUNCH_BLOCKING:\", os.environ[\"CUDA_LAUNCH_BLOCKING\"])\n",
    "print(\"[env] TOKENIZERS_PARALLELISM:\", os.environ.get(\"TOKENIZERS_PARALLELISM\"))\n",
    "print(\"[cache] HF_HOME:\", os.environ[\"HF_HOME\"])\n",
    "print(\"[cache] HUGGINGFACE_HUB_CACHE:\", os.environ[\"HUGGINGFACE_HUB_CACHE\"])\n",
    "print(\"[cache] TRANSFORMERS_CACHE:\", os.environ[\"TRANSFORMERS_CACHE\"])\n",
    "print(\"[cache] HF_DATASETS_CACHE:\", os.environ[\"HF_DATASETS_CACHE\"])\n",
    "print(\"[cache] CACHE_DIR:\", CACHE_DIR)\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Config\n",
    "# ----------------------------\n",
    "import torch\n",
    "\n",
    "TRAIN_CSV = \"/\"\n",
    "EVAL_CSV  = \"/\"\n",
    "IMG_ROOT  = \"\"\n",
    "\n",
    "MODEL_ID = \"Qwen/Qwen2.5-VL-3B-Instruct\"\n",
    "\n",
    "OUTPUT_DIR = str(Path(HF_CACHE_ROOT) / \"finetunes\" / \"qwen2vl-regression-out\")\n",
    "Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# training (UPDATED)\n",
    "EPOCHS = 2\n",
    "PER_DEVICE_BATCH_SIZE = 4\n",
    "GRAD_ACCUM_STEPS = 8\n",
    "\n",
    "LR = 5e-5                 # was 2e-4\n",
    "WARMUP_RATIO = 0.10       # was 0.03\n",
    "WEIGHT_DECAY = 0.01       # was 0.0\n",
    "MAX_STEPS = -1\n",
    "\n",
    "# prompt + truncation (KEEP 2048 to avoid image-token mismatch)\n",
    "PROMPT_PREFIX = \"Predict the alignment score (a real number). Caption:\"\n",
    "MAX_TEXT_LEN = 2048\n",
    "\n",
    "# LoRA (UPDATED)\n",
    "LORA_R = 16\n",
    "LORA_ALPHA = 32\n",
    "LORA_DROPOUT = 0.10       # was 0.05\n",
    "\n",
    "SEED = 42\n",
    "NUM_WORKERS = 0           # was 2 (avoid tokenizers fork issues)\n",
    "\n",
    "print(\"[cfg] OUTPUT_DIR:\", OUTPUT_DIR)\n",
    "print(\"[cfg] CUDA:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"[cfg] GPU:\", torch.cuda.get_device_name(0))\n",
    "    print(\"[cfg] device_count:\", torch.cuda.device_count())\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Load dataset\n",
    "# ----------------------------\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"[data] Loading CSV(s)...\")\n",
    "data_files = {\"train\": TRAIN_CSV}\n",
    "if EVAL_CSV:\n",
    "    data_files[\"eval\"] = EVAL_CSV\n",
    "\n",
    "raw = load_dataset(\"csv\", data_files=data_files, cache_dir=CACHE_DIR)\n",
    "\n",
    "print(\"[data] splits:\", list(raw.keys()))\n",
    "print(\"[data] columns:\", raw[\"train\"].column_names)\n",
    "print(\"[data] train rows:\", len(raw[\"train\"]))\n",
    "if EVAL_CSV:\n",
    "    print(\"[data] eval rows:\", len(raw[\"eval\"]))\n",
    "\n",
    "required_cols = {\"img_name\", \"caption\", \"alignment_score\"}\n",
    "for split in raw.keys():\n",
    "    missing = required_cols - set(raw[split].column_names)\n",
    "    if missing:\n",
    "        raise ValueError(f\"{split} is missing columns: {missing}\")\n",
    "\n",
    "print(\"[data] example row:\", raw[\"train\"][0])\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Torch dataset (image load on the fly)\n",
    "# ----------------------------\n",
    "import math\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "def load_image(path: str) -> Image.Image:\n",
    "    with Image.open(path) as im:\n",
    "        return im.convert(\"RGB\")\n",
    "\n",
    "class CSVImageCaptionRegressionDataset(Dataset):\n",
    "    def __init__(self, hf_dataset, img_root: str):\n",
    "        self.ds = hf_dataset\n",
    "        self.img_root = img_root\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.ds[idx]\n",
    "        img_path = os.path.join(self.img_root, str(row[\"img_name\"]))\n",
    "        caption = str(row[\"caption\"])\n",
    "        label = float(row[\"alignment_score\"])\n",
    "\n",
    "        if not math.isfinite(label):\n",
    "            raise ValueError(f\"Non-finite label at idx={idx}: {label}\")\n",
    "\n",
    "        image = load_image(img_path)\n",
    "        return {\"image\": image, \"caption\": caption, \"labels\": label, \"img_path\": img_path}\n",
    "\n",
    "train_ds = CSVImageCaptionRegressionDataset(raw[\"train\"], IMG_ROOT)\n",
    "eval_ds  = CSVImageCaptionRegressionDataset(raw[\"eval\"], IMG_ROOT) if EVAL_CSV else None\n",
    "print(\"[ds] train:\", len(train_ds), \"| eval:\", (len(eval_ds) if eval_ds else 0))\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Load processor + base model + regression head + LoRA\n",
    "# ----------------------------\n",
    "from transformers import AutoProcessor, AutoModel\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "print(\"[model] Loading processor...\")\n",
    "processor = AutoProcessor.from_pretrained(MODEL_ID, trust_remote_code=True, cache_dir=CACHE_DIR)\n",
    "\n",
    "print(\"[model] Loading base model (fp16)...\")\n",
    "base = AutoModel.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    trust_remote_code=True,\n",
    "    cache_dir=CACHE_DIR,\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "print(\"[model] Base loaded.\")\n",
    "\n",
    "class Qwen2VLForRegression(nn.Module):\n",
    "    \"\"\"\n",
    "    Trainer-friendly regression wrapper:\n",
    "      - returns dict with keys: loss, logits\n",
    "      - logits shape: [B, 1]\n",
    "    Stability:\n",
    "      - pooling in fp32\n",
    "      - regression head run in fp32\n",
    "      - mse loss in fp32\n",
    "    \"\"\"\n",
    "    def __init__(self, base_model: nn.Module):\n",
    "        super().__init__()\n",
    "        self.base = base_model\n",
    "        hidden_size = getattr(base_model.config, \"hidden_size\", None)\n",
    "        if hidden_size is None:\n",
    "            hidden_size = getattr(getattr(base_model.config, \"text_config\", None), \"hidden_size\", None)\n",
    "        if hidden_size is None:\n",
    "            raise ValueError(\"Could not infer hidden_size from model config.\")\n",
    "        self.reg_head = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, labels=None, **inputs):\n",
    "        base_inputs = {k: v for k, v in inputs.items() if torch.is_tensor(v) and k != \"labels\"}\n",
    "\n",
    "        out = self.base(**base_inputs, output_hidden_states=False, return_dict=True)\n",
    "        last_hidden = out.last_hidden_state  # [B, T, H]\n",
    "\n",
    "        attn = base_inputs.get(\"attention_mask\", None)\n",
    "        if attn is None:\n",
    "            pooled = last_hidden.float().mean(dim=1)\n",
    "        else:\n",
    "            attn_f = attn.float()\n",
    "            lh_f = last_hidden.float()\n",
    "            denom = attn_f.sum(dim=1).clamp_min(1.0).unsqueeze(-1)\n",
    "            pooled = (lh_f * attn_f.unsqueeze(-1)).sum(dim=1) / denom\n",
    "\n",
    "        logits = self.reg_head(pooled).float()  # [B, 1] fp32\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = F.mse_loss(logits.squeeze(-1), labels.float())\n",
    "\n",
    "        return {\"loss\": loss, \"logits\": logits}\n",
    "\n",
    "model = Qwen2VLForRegression(base)\n",
    "\n",
    "print(\"[lora] Applying LoRA (FEATURE_EXTRACTION)...\")\n",
    "target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    "lora_cfg = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    target_modules=target_modules,\n",
    "    task_type=TaskType.FEATURE_EXTRACTION,\n",
    ")\n",
    "model.base = get_peft_model(model.base, lora_cfg)\n",
    "model.base.print_trainable_parameters()\n",
    "\n",
    "for p in model.reg_head.parameters():\n",
    "    p.requires_grad = True\n",
    "print(\"[model] reg_head params:\", sum(p.numel() for p in model.reg_head.parameters()))\n",
    "\n",
    "# ----------------------------\n",
    "# 6) Collator (keep your original truncation behavior)\n",
    "# ----------------------------\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "class Qwen2VLRegressionCollator:\n",
    "    def __init__(self, processor, prompt_prefix: str, max_text_len: int, debug: bool = False):\n",
    "        self.processor = processor\n",
    "        self.prompt_prefix = prompt_prefix\n",
    "        self.max_text_len = max_text_len\n",
    "        self.debug = debug\n",
    "        self._seen = 0\n",
    "\n",
    "        self.vocab_size = None\n",
    "        tok = getattr(processor, \"tokenizer\", None)\n",
    "        if tok is not None and hasattr(tok, \"__len__\"):\n",
    "            self.vocab_size = int(len(tok))  # includes added/special tokens\n",
    "\n",
    "    def __call__(self, batch: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        images = [ex[\"image\"] for ex in batch]\n",
    "        captions = [ex[\"caption\"] for ex in batch]\n",
    "        labels = torch.tensor([float(ex[\"labels\"]) for ex in batch], dtype=torch.float32)\n",
    "\n",
    "        texts = []\n",
    "        for cap in captions:\n",
    "            prompt = f\"{self.prompt_prefix}\\n{cap}\\nScore:\"\n",
    "            messages = [{\"role\": \"user\", \"content\": [{\"type\": \"image\"}, {\"type\": \"text\", \"text\": prompt}]}]\n",
    "            text = self.processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "            texts.append(text)\n",
    "\n",
    "        model_inputs = self.processor(\n",
    "            text=texts,\n",
    "            images=images,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=self.max_text_len,   # keep 2048 to avoid mismatch\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        model_inputs[\"labels\"] = labels\n",
    "\n",
    "        if \"input_ids\" in model_inputs:\n",
    "            max_id = int(model_inputs[\"input_ids\"].max())\n",
    "            if self.vocab_size is not None and max_id >= self.vocab_size:\n",
    "                raise ValueError(f\"input_ids has id {max_id} >= vocab_size {self.vocab_size}\")\n",
    "\n",
    "        if self.debug and self._seen < 3:\n",
    "            print(\"[collator] prompt preview:\\n\", texts[0][:400], \"...\")\n",
    "            print(\"[collator] input_ids:\", tuple(model_inputs[\"input_ids\"].shape))\n",
    "            print(\"[collator] labels:\", labels[: min(4, len(labels))].tolist())\n",
    "            self._seen += 1\n",
    "\n",
    "        return model_inputs\n",
    "\n",
    "collator = Qwen2VLRegressionCollator(processor, PROMPT_PREFIX, MAX_TEXT_LEN, debug=False)\n",
    "\n",
    "b = collator([train_ds[0]])\n",
    "print(\"[collator] keys:\", list(b.keys()))\n",
    "\n",
    "# ----------------------------\n",
    "# 7) Trainer + progress prints + resume\n",
    "# ----------------------------\n",
    "from transformers import TrainingArguments, Trainer, set_seed, TrainerCallback\n",
    "import numpy as np\n",
    "import time\n",
    "import re\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    preds, labels = eval_pred\n",
    "    preds = np.asarray(preds, dtype=np.float32).squeeze()   # handles [N,1] or [N]\n",
    "    labels = np.asarray(labels, dtype=np.float32).squeeze()\n",
    "\n",
    "    bad = ~np.isfinite(preds)\n",
    "    if bad.any():\n",
    "        return {\n",
    "            \"mse\": float(\"nan\"),\n",
    "            \"mae\": float(\"nan\"),\n",
    "            \"r2\": float(\"nan\"),\n",
    "            \"pred_nan_frac\": float(bad.mean()),\n",
    "        }\n",
    "\n",
    "    mse = float(np.mean((preds - labels) ** 2))\n",
    "    mae = float(np.mean(np.abs(preds - labels)))\n",
    "    var = float(np.var(labels))\n",
    "    r2 = float(\"nan\") if var == 0.0 else float(1.0 - mse / var)\n",
    "    return {\"mse\": mse, \"mae\": mae, \"r2\": r2}\n",
    "\n",
    "class PrintProgressCallback(TrainerCallback):\n",
    "    def __init__(self, trainer_ref=None, sample_every_steps: int = 100):\n",
    "        self.trainer_ref = trainer_ref\n",
    "        self.sample_every_steps = sample_every_steps\n",
    "        self.t0 = None\n",
    "\n",
    "    def on_train_begin(self, args, state, control, **kwargs):\n",
    "        self.t0 = time.time()\n",
    "        print(f\"[train] begin | max_steps={state.max_steps} | epochs={args.num_train_epochs}\")\n",
    "\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if not logs:\n",
    "            return\n",
    "        elapsed = (time.time() - self.t0) if self.t0 else 0.0\n",
    "        msg = f\"[log] step={state.global_step} epoch={state.epoch:.3f} elapsed={elapsed/60:.1f}m\"\n",
    "        loss_val = logs.get(\"loss\", logs.get(\"train_loss\", None))\n",
    "        if loss_val is not None:\n",
    "            msg += f\" loss={loss_val:.4f}\"\n",
    "        if \"learning_rate\" in logs:\n",
    "            msg += f\" lr={logs['learning_rate']:.2e}\"\n",
    "        if \"grad_norm\" in logs:\n",
    "            msg += f\" grad_norm={logs['grad_norm']:.3f}\"\n",
    "        print(msg)\n",
    "\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        if self.trainer_ref is None:\n",
    "            return\n",
    "        if state.global_step > 0 and (state.global_step % self.sample_every_steps == 0):\n",
    "            try:\n",
    "                m = self.trainer_ref.model\n",
    "                m.eval()\n",
    "                ex = train_ds[0]\n",
    "                batch = collator([ex])\n",
    "                device = next(m.parameters()).device\n",
    "                batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n",
    "                with torch.no_grad():\n",
    "                    out = m(**batch)\n",
    "                pred = float(out[\"logits\"].detach().float().cpu().squeeze().item())\n",
    "                print(f\"[sample] step={state.global_step} pred={pred:.4f} label={float(ex['labels']):.4f}\")\n",
    "            except Exception as e:\n",
    "                print(\"[sample] failed:\", repr(e))\n",
    "\n",
    "def get_latest_checkpoint(output_dir: str) -> Optional[str]:\n",
    "    out = Path(output_dir)\n",
    "    if not out.exists():\n",
    "        return None\n",
    "    ckpts = [p for p in out.glob(\"checkpoint-*\") if p.is_dir()]\n",
    "    if not ckpts:\n",
    "        return None\n",
    "\n",
    "    def step_num(p: Path) -> int:\n",
    "        m = re.search(r\"checkpoint-(\\d+)\", p.name)\n",
    "        return int(m.group(1)) if m else -1\n",
    "\n",
    "    return str(max(ckpts, key=step_num))\n",
    "\n",
    "# Optim: fused if available, else fallback\n",
    "try:\n",
    "    _OPTIM = \"adamw_torch_fused\"\n",
    "except Exception:\n",
    "    _OPTIM = \"adamw_torch\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=PER_DEVICE_BATCH_SIZE,\n",
    "    per_device_eval_batch_size=PER_DEVICE_BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRAD_ACCUM_STEPS,\n",
    "    learning_rate=LR,\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    logging_steps=10,\n",
    "\n",
    "    save_steps=200,\n",
    "    save_total_limit=2,\n",
    "\n",
    "    # NOTE: if your transformers complains, rename eval_strategy -> evaluation_strategy\n",
    "    eval_strategy=\"steps\" if eval_ds is not None else \"no\",\n",
    "    eval_steps=200 if eval_ds is not None else None,\n",
    "\n",
    "    bf16=False,\n",
    "    fp16=True,\n",
    "\n",
    "    dataloader_num_workers=NUM_WORKERS,\n",
    "    remove_unused_columns=False,\n",
    "    max_steps=MAX_STEPS,\n",
    "    report_to=\"none\",\n",
    "\n",
    "    # Stability knobs (IMPORTANT)\n",
    "    max_grad_norm=1.0,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    optim=_OPTIM,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    data_collator=collator,\n",
    "    compute_metrics=compute_metrics if eval_ds is not None else None,\n",
    ")\n",
    "\n",
    "trainer.add_callback(PrintProgressCallback(trainer_ref=trainer, sample_every_steps=100))\n",
    "\n",
    "latest_ckpt = get_latest_checkpoint(OUTPUT_DIR)\n",
    "print(\"[resume] latest_ckpt:\", latest_ckpt)\n",
    "\n",
    "# Sanity print (so you know you're running the new hyperparams)\n",
    "print(\"[sanity] trainer.args.learning_rate:\", trainer.args.learning_rate)\n",
    "print(\"[sanity] trainer.args.warmup_ratio:\", trainer.args.warmup_ratio)\n",
    "print(\"[sanity] trainer.args.weight_decay:\", trainer.args.weight_decay)\n",
    "print(\"[sanity] trainer.args.max_grad_norm:\", trainer.args.max_grad_norm)\n",
    "print(\"[sanity] trainer.args.lr_scheduler_type:\", trainer.args.lr_scheduler_type)\n",
    "print(\"[sanity] trainer.args.optim:\", trainer.args.optim)\n",
    "\n",
    "print(\"[train] Starting training...\")\n",
    "trainer.train(resume_from_checkpoint=latest_ckpt if latest_ckpt else None)\n",
    "print(\"[train] Training finished.\")\n",
    "\n",
    "# ----------------------------\n",
    "# 8) Save\n",
    "# ----------------------------\n",
    "print(\"[save] Saving full model + processor to:\", OUTPUT_DIR)\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "processor.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "lora_dir = str(Path(OUTPUT_DIR) / \"lora_adapter\")\n",
    "print(\"[save] Saving LoRA adapter to:\", lora_dir)\n",
    "model.base.save_pretrained(lora_dir)\n",
    "\n",
    "print(\"[save] Done.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
